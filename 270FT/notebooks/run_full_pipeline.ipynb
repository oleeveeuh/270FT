{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning LLMs: Complete Pipeline\n",
        "\n",
        "This notebook runs the complete fine-tuning and evaluation pipeline for fine-tuning LLMs.\n",
        "\n",
        "**Optimized for 8GB VRAM** with memory-efficient settings:\n",
        "- 4-bit quantization (QLoRA)\n",
        "- Batch size: 1 with gradient accumulation\n",
        "- Gradient checkpointing enabled\n",
        "- Reduced sequence length: 1024 tokens\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Setup**: Check system and install dependencies\n",
        "2. **Data Preparation**: Verify training data is ready\n",
        "3. **Training**: Fine-tune Qwen 2.5 7B with QLoRA (memory-optimized)\n",
        "4. **Evaluation**: Evaluate models on test set\n",
        "5. **Interactive Demo**: Load model and generate responses\n",
        "\n",
        "## Prerequisites\n",
        "- CUDA-capable GPU with 8GB+ VRAM\n",
        "- Python 3.8+\n",
        "- Training data in `data/processed/` (or run preprocessing first)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability and memory\n",
        "import torch\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COLAB RUNTIME INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"\\n[OK] Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"\\n[WARNING] Not running in Colab - this notebook is optimized for Colab\")\n",
        "\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"GPU Total Memory: {total_memory:.2f} GB\")\n",
        "    \n",
        "    # Check current memory usage\n",
        "    torch.cuda.empty_cache()\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    print(f\"GPU Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"GPU Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"GPU Free: {total_memory - reserved:.2f} GB\")\n",
        "    \n",
        "    # Colab-specific recommendations\n",
        "    if \"T4\" in gpu_name:\n",
        "        print(\"\\n[OK] T4 GPU detected (Colab Free/Standard)\")\n",
        "        print(\"  Recommended settings: batch_size=2, gradient_accumulation=2\")\n",
        "    elif \"V100\" in gpu_name:\n",
        "        print(\"\\n[OK] V100 GPU detected (Colab Pro)\")\n",
        "        print(\"  Recommended settings: batch_size=4, gradient_accumulation=2\")\n",
        "    elif \"A100\" in gpu_name:\n",
        "        print(\"\\n[OK] A100 GPU detected (Colab Pro+)\")\n",
        "        print(\"  Recommended settings: batch_size=8, gradient_accumulation=1\")\n",
        "    elif total_memory < 10:\n",
        "        print(\"\\n[WARNING] GPU has less than 10GB VRAM.\")\n",
        "        print(\"   Training will use aggressive memory optimizations:\")\n",
        "        print(\"   - Batch size: 1\")\n",
        "        print(\"   - Gradient accumulation: 4\")\n",
        "        print(\"   - Gradient checkpointing: enabled\")\n",
        "        print(\"   - Max sequence length: 1024\")\n",
        "else:\n",
        "    print(\"[WARNING] No CUDA GPU detected!\")\n",
        "    print(\"   Please enable GPU: Runtime → Change runtime type → GPU\")\n",
        "    print(\"   Training will be very slow on CPU.\")\n",
        "\n",
        "# System RAM\n",
        "ram = psutil.virtual_memory()\n",
        "print(f\"\\nSystem RAM: {ram.total / 1e9:.2f} GB\")\n",
        "print(f\"System RAM Available: {ram.available / 1e9:.2f} GB\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    if ram.total / 1e9 < 15:\n",
        "        print(\"\\n[INFO] Tip: You can request more RAM (up to 25GB) if needed:\")\n",
        "        print(\"   Runtime → Change runtime type → High-RAM\")\n",
        "    print(f\"\\nColab Session: Free tier typically ~12GB RAM, Pro up to 52GB\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository and setup paths for Colab\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Colab: Clone from GitHub\n",
        "    REPO_URL = \"https://github.com/oleeveeuh/270FT.git\"  # UPDATE THIS with your repo URL!\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"CLONING REPOSITORY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n[IMPORTANT] Update REPO_URL in this cell with your repository!\")\n",
        "    print(f\"   Current: {REPO_URL}\")\n",
        "    print(f\"\\nCloning repository...\")\n",
        "    \n",
        "    # If the repo directory doesn't exist in the current working directory, clone it.\n",
        "    if not os.path.exists(\"270FT\"):\n",
        "        result = subprocess.run(\n",
        "            [\"git\", \"clone\", REPO_URL],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        if result.returncode != 0:\n",
        "            print(f\"[ERROR] Failed to clone repository:\")\n",
        "            print(result.stderr)\n",
        "            raise RuntimeError(\"Please update REPO_URL with your repository\")\n",
        "        print(\"[OK] Repository cloned\")\n",
        "    else:\n",
        "        print(\"Repository already exists, skipping clone\")\n",
        "        # If repository exists, try to pull latest changes\n",
        "        try:\n",
        "            subprocess.run([\"git\", \"-C\", \"270FT\", \"pull\"], check=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Resolve project_root robustly to avoid duplicate segments like /content/270FT/270FT\n",
        "    cwd = Path.cwd()\n",
        "    if cwd.name == \"270FT\":\n",
        "        project_root = cwd\n",
        "    elif (cwd / \"270FT\").exists():\n",
        "        project_root = cwd / \"270FT\"\n",
        "    else:\n",
        "        # Fallback: search for a directory named '270FT' under cwd\n",
        "        matches = list(cwd.glob(\"**/270FT\"))\n",
        "        project_root = matches[0] if matches else cwd\n",
        "\n",
        "    os.chdir(project_root)\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "else:\n",
        "    # Local: Use existing project\n",
        "    notebook_dir = Path.cwd()\n",
        "    # If the current working dir is inside a `notebooks` folder, assume repo root is its parent\n",
        "    if \"notebooks\" in str(notebook_dir):\n",
        "        project_root = notebook_dir.parent\n",
        "    else:\n",
        "        project_root = notebook_dir\n",
        "    print(f\"Local execution - Project root: {project_root}\")\n",
        "\n",
        "# Add project to path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"\\n[OK] Project paths configured\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"INSTALLING DEPENDENCIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "requirements_file = project_root / \"requirements.txt\"\n",
        "if requirements_file.exists():\n",
        "    print(f\"\\nInstalling from requirements.txt...\")\n",
        "    result = subprocess.run(\n",
        "        [\"pip\", \"install\", \"-q\", \"-r\", str(requirements_file)],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"[OK] Dependencies installed from requirements.txt\")\n",
        "    else:\n",
        "        print(f\"[WARNING] Some packages may have failed to install\")\n",
        "        print(\"Installing essential packages individually...\")\n",
        "        packages = [\n",
        "            \"torch\", \"transformers\", \"peft\", \"bitsandbytes\", \n",
        "            \"datasets\", \"accelerate\", \"sympy\", \"evaluate\",\n",
        "            \"wandb\", \"pyyaml\", \"psutil\"\n",
        "        ]\n",
        "        for pkg in packages:\n",
        "            subprocess.run([\"pip\", \"install\", \"-q\", pkg], check=False)\n",
        "else:\n",
        "    print(f\"[INFO] requirements.txt not found, installing essential packages...\")\n",
        "    packages = [\n",
        "        \"torch\", \"transformers\", \"peft\", \"bitsandbytes\", \n",
        "        \"datasets\", \"accelerate\", \"sympy\", \"evaluate\",\n",
        "        \"wandb\", \"pyyaml\", \"psutil\", \"pdfplumber\", \"pypdf\"\n",
        "    ]\n",
        "    for pkg in packages:\n",
        "        subprocess.run([\"pip\", \"install\", \"-q\", pkg], check=False)\n",
        "    print(\"[OK] Essential packages installed\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "try:\n",
        "    import transformers\n",
        "    import peft\n",
        "    import sympy\n",
        "    import z3\n",
        "    import wandb\n",
        "    import yaml\n",
        "    print(\"[OK] All required packages installed successfully\")\n",
        "    print(f\"  - Transformers: {transformers.__version__}\")\n",
        "    print(f\"  - PEFT: {peft.__version__}\")\n",
        "    print(f\"  - SymPy: {sympy.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"[ERROR] Import error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directories if they don't exist\n",
        "from pathlib import Path\n",
        "\n",
        "# Use project root from previous cell\n",
        "data_raw = project_root / \"data\" / \"raw\"\n",
        "data_processed = project_root / \"data\" / \"processed\"\n",
        "\n",
        "data_raw.mkdir(parents=True, exist_ok=True)\n",
        "data_processed.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data directories:\")\n",
        "print(f\"  - Raw: {data_raw}\")\n",
        "print(f\"  - Processed: {data_processed}\")\n",
        "print(f\"[OK] Directories ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create sample training data if it doesn't exist\n",
        "# In practice, you would upload your own data files\n",
        "\n",
        "import json\n",
        "\n",
        "sample_train_data = [\n",
        "    {\n",
        "        \"prompt\": \"Prove that the sum of the first n natural numbers is n(n+1)/2\",\n",
        "        \"response\": \"[Algorithm Outline]\\nUse mathematical induction to prove the formula.\\n\\n[Pseudocode]\\nfunction verify_sum(n):\\n    if n == 1:\\n        return 1 == 1 * 2 / 2  // Base case\\n    // Inductive step: assume true for k, prove for k+1\\n\\n[Proof Summary]\\nBase case (n=1): Sum = 1, formula = 1(2)/2 = 1\\nInductive step: Assume sum(1..k) = k(k+1)/2.\\nFor k+1: sum(1..k+1) = k(k+1)/2 + (k+1) = (k+1)(k+2)/2\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Explain the binary search algorithm\",\n",
        "        \"response\": \"[Algorithm Outline]\\nBinary search finds an element in a sorted array by repeatedly dividing the search space in half.\\n\\n[Pseudocode]\\nfunction binary_search(arr, target):\\n    left = 0\\n    right = len(arr) - 1\\n    while left <= right:\\n        mid = (left + right) // 2\\n        if arr[mid] == target:\\n            return mid\\n        elif arr[mid] < target:\\n            left = mid + 1\\n        else:\\n            right = mid - 1\\n    return -1\\n\\n[Proof Summary]\\nTime complexity: O(log n) because we halve the search space each iteration.\\nSpace complexity: O(1) for iterative version.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "sample_test_data = [\n",
        "    {\n",
        "        \"prompt\": \"Prove that 1 + 2 + ... + n = n(n+1)/2\",\n",
        "        \"response\": \"[Algorithm Outline]\\nMathematical induction proof.\\n\\n[Pseudocode]\\nBase: n=1 → 1 = 1(2)/2 = 1\\nInductive: sum(1..k+1) = sum(1..k) + (k+1) = k(k+1)/2 + (k+1) = (k+1)(k+2)/2\\n\\n[Proof Summary]\\nBy mathematical induction, the formula holds for all natural numbers n.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save sample data (only if files don't exist)\n",
        "train_path = data_raw / \"train.json\"\n",
        "test_path = data_raw / \"test.json\"\n",
        "\n",
        "if not train_path.exists():\n",
        "    with open(train_path, \"w\") as f:\n",
        "        json.dump(sample_train_data, f, indent=2)\n",
        "    print(f\"[OK] Created sample training data: {train_path}\")\n",
        "else:\n",
        "    print(f\"Training data already exists: {train_path}\")\n",
        "\n",
        "if not test_path.exists():\n",
        "    with open(test_path, \"w\") as f:\n",
        "        json.dump(sample_test_data, f, indent=2)\n",
        "    print(f\"[OK] Created sample test data: {test_path}\")\n",
        "else:\n",
        "    print(f\"Test data already exists: {test_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for processed data (preferred) or raw data\n",
        "processed_train = data_processed / \"train.jsonl\"\n",
        "processed_val = data_processed / \"validation.jsonl\"\n",
        "processed_test = data_processed / \"test.jsonl\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for processed data first\n",
        "if processed_train.exists():\n",
        "    print(f\"\\n[OK] Found processed training data: {processed_train}\")\n",
        "    with open(processed_train, \"r\") as f:\n",
        "        train_count = sum(1 for line in f if line.strip())\n",
        "    print(f\"  Training samples: {train_count}\")\n",
        "else:\n",
        "    print(f\"\\n[INFO] No processed training data found at {processed_train}\")\n",
        "    print(\"  Will look for raw data or you may need to run preprocessing first\")\n",
        "\n",
        "if processed_val.exists():\n",
        "    with open(processed_val, \"r\") as f:\n",
        "        val_count = sum(1 for line in f if line.strip())\n",
        "    print(f\"  Validation samples: {val_count}\")\n",
        "else:\n",
        "    print(f\"  [INFO] No validation data found (optional)\")\n",
        "\n",
        "if processed_test.exists():\n",
        "    with open(processed_test, \"r\") as f:\n",
        "        test_count = sum(1 for line in f if line.strip())\n",
        "    print(f\"  Test samples: {test_count}\")\n",
        "else:\n",
        "    print(f\"  [WARNING] No test data found at {processed_test}\")\n",
        "    print(\"  Training requires test data. Please add test.jsonl to data/processed/\")\n",
        "\n",
        "# Also check raw data\n",
        "print(f\"\\nRaw data directory: {data_raw}\")\n",
        "if data_raw.exists() and any(data_raw.iterdir()):\n",
        "    raw_files = list(data_raw.iterdir())\n",
        "    print(f\"  Raw files found: {len(raw_files)}\")\n",
        "    if not processed_train.exists():\n",
        "        print(\"  [WARNING] You may need to run preprocessing first:\")\n",
        "        print(\"     python 270FT/preprocess/load_and_prepare.py\")\n",
        "        print(\"     or: python cli.py preprocess\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preflight: run preprocessing if processed data missing\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "processed_train = data_processed / \"train.jsonl\"\n",
        "processed_val = data_processed / \"validation.jsonl\"\n",
        "processed_test = data_processed / \"test.jsonl\"\n",
        "\n",
        "missing = [p for p in (processed_train, processed_val, processed_test) if not p.exists()]\n",
        "preprocess_script = project_root / \"preprocess\" / \"load_and_prepare.py\"\n",
        "\n",
        "if not missing:\n",
        "    print(f\"[OK] All processed data present: {processed_train}, {processed_val}, {processed_test}\")\n",
        "else:\n",
        "    print(f\"[INFO] Missing processed files: {[str(p) for p in missing]}\")\n",
        "    # Try to run preprocessing script if available\n",
        "    if preprocess_script.exists():\n",
        "        print(f\"[INFO] Running preprocessing script: {preprocess_script}\")\n",
        "        result = subprocess.run([sys.executable, str(preprocess_script)], cwd=str(project_root), capture_output=False, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"[OK] Preprocessing completed successfully\")\n",
        "        else:\n",
        "            print(f\"[ERROR] Preprocessing failed with exit code {result.returncode}\")\n",
        "            raise RuntimeError(\"Preprocessing failed. Check the output above.\")\n",
        "    else:\n",
        "        print(f\"[ERROR] Preprocessing script not found: {preprocess_script}\")\n",
        "        print(\"Please run preprocessing manually: python preprocess/load_and_prepare.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display and optionally adjust training configuration for Colab\n",
        "import yaml\n",
        "\n",
        "config_path = project_root / \"configs\" / \"training_config.yaml\"\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModels to train: {[m['name'] for m in config['models']]}\")\n",
        "print(f\"\\nTraining Parameters:\")\n",
        "print(f\"  Epochs: {config['training']['epochs']}\")\n",
        "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"  Gradient accumulation steps: {config['training'].get('gradient_accumulation_steps', 1)}\")\n",
        "print(f\"  Effective batch size: {config['training']['batch_size'] * config['training'].get('gradient_accumulation_steps', 1)}\")\n",
        "print(f\"  Max sequence length: {config['training'].get('max_length', 2048)}\")\n",
        "print(f\"  Gradient checkpointing: {config['training'].get('gradient_checkpointing', False)}\")\n",
        "print(f\"\\nLoRA Parameters:\")\n",
        "print(f\"  LoRA rank: {config['training']['lora_r']}\")\n",
        "print(f\"  LoRA alpha: {config['training']['lora_alpha']}\")\n",
        "print(f\"  LoRA dropout: {config['training']['lora_dropout']}\")\n",
        "\n",
        "# Colab-specific recommendations\n",
        "if IN_COLAB and torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    current_batch = config['training']['batch_size']\n",
        "    \n",
        "    print(f\"\\n[INFO] Colab GPU Recommendations:\")\n",
        "    if gpu_memory >= 15:  # T4 or better\n",
        "        if current_batch == 1:\n",
        "            print(f\"   Your GPU has {gpu_memory:.1f}GB VRAM - you can increase batch_size to 2\")\n",
        "            print(f\"   Edit config: batch_size: 2, gradient_accumulation_steps: 2\")\n",
        "    elif gpu_memory >= 30:  # A100\n",
        "        if current_batch <= 2:\n",
        "            print(f\"   Your GPU has {gpu_memory:.1f}GB VRAM - you can increase batch_size to 4-8\")\n",
        "            print(f\"   Edit config: batch_size: 4, gradient_accumulation_steps: 1\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Configure W&B for experiment tracking\n",
        "# Uncomment and run if you want to use Weights & Biases\n",
        "\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "# print(\"[OK] W&B configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training script\n",
        "# This will train the configured models (currently Qwen 2.5 7B)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STARTING TRAINING PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n[IMPORTANT] NOTES:\")\n",
        "print(\"  - Training may take several hours depending on your GPU and dataset size\")\n",
        "print(\"  - With 8GB VRAM, expect slower training due to memory optimizations\")\n",
        "print(\"  - Monitor GPU memory usage during training\")\n",
        "print(\"  - If you run out of memory, reduce batch_size or max_length in config\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor GPU memory during training (run this in a separate terminal/notebook if possible)\n",
        "# Or check memory periodically by re-running this cell\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"GPU MEMORY STATUS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total VRAM: {total:.2f} GB\")\n",
        "    print(f\"Allocated: {allocated:.2f} GB ({allocated/total*100:.1f}%)\")\n",
        "    print(f\"Reserved: {reserved:.2f} GB ({reserved/total*100:.1f}%)\")\n",
        "    print(f\"Free: {total - reserved:.2f} GB ({(total-reserved)/total*100:.1f}%)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if reserved / total > 0.9:\n",
        "        print(\"[WARNING] GPU memory usage is very high!\")\n",
        "        print(\"   Consider reducing batch_size or max_length if training fails.\")\n",
        "else:\n",
        "    print(\"No GPU available for monitoring.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute training\n",
        "# This will run the training script directly\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Clear GPU cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU cache cleared\\n\")\n",
        "\n",
        "# Run training script\n",
        "training_script = project_root / \"training\" / \"train_dual_lora.py\"\n",
        "\n",
        "if not training_script.exists():\n",
        "    raise FileNotFoundError(f\"Training script not found: {training_script}\")\n",
        "\n",
        "print(f\"Running training script: {training_script}\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Run as subprocess to capture output properly\n",
        "result = subprocess.run(\n",
        "    [sys.executable, str(training_script)],\n",
        "    cwd=str(project_root),\n",
        "    capture_output=False,  # Show output in real-time\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"[OK] Training completed successfully!\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"[ERROR] Training failed with exit code {result.returncode}\")\n",
        "    print(\"=\"*60)\n",
        "    raise RuntimeError(\"Training failed. Check error messages above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify models were saved\n",
        "models_dir = project_root / \"models\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFYING SAVED MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_models_saved = True\n",
        "for model_config in config[\"models\"]:\n",
        "    model_path = models_dir / model_config[\"output_dir\"]\n",
        "    if model_path.exists():\n",
        "        files = list(model_path.iterdir())\n",
        "        adapter_config = model_path / \"adapter_config.json\"\n",
        "        adapter_weights = model_path / \"adapter_model.safetensors\"\n",
        "        if not adapter_weights.exists():\n",
        "            adapter_weights = model_path / \"adapter_model.bin\"\n",
        "        \n",
        "        print(f\"\\n[OK] {model_config['name']}\")\n",
        "        print(f\"  Path: {model_path}\")\n",
        "        print(f\"  Adapter config: {'OK' if adapter_config.exists() else 'MISSING'}\")\n",
        "        print(f\"  Adapter weights: {'OK' if adapter_weights.exists() else 'MISSING'}\")\n",
        "        print(f\"  Total files: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"\\n[ERROR] {model_config['name']} not found at {model_path}\")\n",
        "        all_models_saved = False\n",
        "\n",
        "if all_models_saved:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"[OK] All models saved successfully!\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"[WARNING] Some models may not have been saved correctly\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation script\n",
        "print(\"Running evaluation on test set...\")\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation script\n",
        "evaluation_script = project_root / \"evaluation\" / \"evaluate_models.py\"\n",
        "\n",
        "if not evaluation_script.exists():\n",
        "    raise FileNotFoundError(f\"Evaluation script not found: {evaluation_script}\")\n",
        "\n",
        "print(f\"Running evaluation script: {evaluation_script}\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear GPU cache before evaluation\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Run as subprocess\n",
        "result = subprocess.run(\n",
        "    [sys.executable, str(evaluation_script)],\n",
        "    cwd=str(project_root),\n",
        "    capture_output=False,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"[OK] Evaluation completed successfully!\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"[ERROR] Evaluation failed with exit code {result.returncode}\")\n",
        "    print(\"=\"*60)\n",
        "    raise RuntimeError(\"Evaluation failed. Check error messages above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display evaluation results\n",
        "results_path = project_root / \"results\" / \"metrics_report.json\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if results_path.exists():\n",
        "    with open(results_path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    if \"model_results\" in results:\n",
        "        for model_name, model_results in results[\"model_results\"].items():\n",
        "            print(f\"\\nModel: {model_name}\")\n",
        "            print(f\"  Exact Match Rate: {model_results.get('exact_match_rate', 0):.4f}\")\n",
        "            print(f\"  Symbolic Equivalence Rate: {model_results.get('symbolic_equivalence_rate', 0):.4f}\")\n",
        "            print(f\"  Average BLEU Score: {model_results.get('avg_bleu_score', 0):.4f}\")\n",
        "    else:\n",
        "        print(\"\\nResults structure:\")\n",
        "        print(json.dumps(results, indent=2))\n",
        "else:\n",
        "    print(f\"\\n[WARNING] Results file not found: {results_path}\")\n",
        "    print(\"Please run evaluation first (Cell 19).\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Interactive Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a model and generate a response\n",
        "# For 8GB VRAM, we'll use 4-bit quantization for inference too\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "def load_model_for_demo(model_name, adapter_path, device=\"cuda\", use_quantization=True):\n",
        "    \"\"\"Load model with adapter for interactive use (memory-optimized).\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    print(f\"  Adapter: {adapter_path}\")\n",
        "    print(f\"  Device: {device}\")\n",
        "    print(f\"  Quantization: {use_quantization}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    \n",
        "    # Use 4-bit quantization for inference on 8GB GPUs\n",
        "    if use_quantization and device == \"cuda\":\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            device_map=\"auto\" if device == \"cuda\" else None,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    \n",
        "    # Load LoRA adapter\n",
        "    model = PeftModel.from_pretrained(model, str(adapter_path))\n",
        "    model.eval()\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    \n",
        "    # Check memory usage\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "        print(f\"  GPU Memory Allocated: {allocated:.2f} GB\")\n",
        "    \n",
        "    print(f\"[OK] Model loaded successfully\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(model, tokenizer, question, max_new_tokens=512, max_length=1024):\n",
        "    \"\"\"Generate response to a question.\"\"\"\n",
        "    prompt = f\"### Question:\\n{question}\\n\\n### Solution:\\n\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    return generated.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load first available model\n",
        "models_dir = project_root / \"models\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Clear GPU cache before loading\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Try to load the first model from config\n",
        "model_loaded = False\n",
        "model = None\n",
        "tokenizer = None\n",
        "current_model_name = None\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MODEL FOR INFERENCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_config in config[\"models\"]:\n",
        "    adapter_path = models_dir / model_config[\"output_dir\"]\n",
        "    \n",
        "    if adapter_path.exists() and (adapter_path / \"adapter_config.json\").exists():\n",
        "        try:\n",
        "            # Use quantization for 8GB GPUs\n",
        "            use_quantization = torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory < 10e9\n",
        "            model, tokenizer = load_model_for_demo(\n",
        "                model_config[\"name\"],\n",
        "                adapter_path,\n",
        "                device=device,\n",
        "                use_quantization=use_quantization\n",
        "            )\n",
        "            model_loaded = True\n",
        "            current_model_name = model_config[\"name\"]\n",
        "            print(f\"\\n[OK] Successfully loaded: {current_model_name}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[ERROR] Failed to load {model_config['name']}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "if not model_loaded:\n",
        "    print(\"\\n[ERROR] No trained models found. Please run training first (Cell 15).\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample question\n",
        "if model_loaded:\n",
        "    test_question = \"Prove that the sum of the first n natural numbers is n(n+1)/2\"\n",
        "    \n",
        "    print(f\"Question: {test_question}\\n\")\n",
        "    print(\"Generating response...\\n\")\n",
        "    \n",
        "    response = generate_response(model, tokenizer, test_question)\n",
        "    \n",
        "    print(\"Response:\")\n",
        "    print(\"=\"*60)\n",
        "    print(response)\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Query Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive cell - modify the question and run\n",
        "if model_loaded:\n",
        "    # Change this question to test different queries\n",
        "    your_question = \"Explain the quicksort algorithm\"\n",
        "    \n",
        "    print(f\"Question: {your_question}\\n\")\n",
        "    print(\"Generating response...\\n\")\n",
        "    \n",
        "    response = generate_response(model, tokenizer, your_question, max_new_tokens=1024)\n",
        "    \n",
        "    print(\"Response:\")\n",
        "    print(\"=\"*60)\n",
        "    print(response)\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"Please load a model first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Download Models (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save models to Google Drive (persistent storage)\n",
        "# This keeps your models even after Colab session ends\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        import shutil\n",
        "        \n",
        "        print(\"=\"*60)\n",
        "        print(\"SAVING TO GOOGLE DRIVE\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Mount Google Drive\n",
        "        print(\"\\nMounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "        \n",
        "        # Create directory in Drive\n",
        "        drive_models_dir = Path(\"/content/drive/MyDrive/270FT_models\")\n",
        "        drive_models_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Copy models\n",
        "        models_dir = project_root / \"models\"\n",
        "        if models_dir.exists():\n",
        "            print(f\"\\nCopying models to Google Drive...\")\n",
        "            print(f\"  From: {models_dir}\")\n",
        "            print(f\"  To: {drive_models_dir}\")\n",
        "            \n",
        "            # Copy each model directory\n",
        "            for model_subdir in models_dir.iterdir():\n",
        "                if model_subdir.is_dir():\n",
        "                    dest = drive_models_dir / model_subdir.name\n",
        "                    if dest.exists():\n",
        "                        shutil.rmtree(dest)\n",
        "                    shutil.copytree(model_subdir, dest)\n",
        "                    print(f\"  [OK] Copied: {model_subdir.name}\")\n",
        "            \n",
        "            print(f\"\\n[OK] Models saved to Google Drive!\")\n",
        "            print(f\"  Location: {drive_models_dir}\")\n",
        "        else:\n",
        "            print(\"[WARNING] No models found to copy\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"[INFO] Google Drive save failed: {e}\")\n",
        "        print(\"  You can manually copy models or use the download option above\")\n",
        "else:\n",
        "    print(\"Not running in Colab - skipping Google Drive save\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained models (Colab)\n",
        "# This creates a compressed archive and downloads it\n",
        "\n",
        "if IN_COLAB:\n",
        "    import shutil\n",
        "    from datetime import datetime\n",
        "    \n",
        "    models_dir = project_root / \"models\"\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"DOWNLOADING TRAINED MODELS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if models_dir.exists() and any(models_dir.iterdir()):\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        archive_name = f\"trained_models_{timestamp}\"\n",
        "        \n",
        "        print(f\"\\nCreating archive...\")\n",
        "        archive_path = Path(\"/content\") / f\"{archive_name}.zip\"\n",
        "        shutil.make_archive(\n",
        "            str(Path(\"/content\") / archive_name),\n",
        "            \"zip\",\n",
        "            models_dir\n",
        "        )\n",
        "        \n",
        "        archive_size = archive_path.stat().st_size / 1e9\n",
        "        print(f\"[OK] Archive created: {archive_path}\")\n",
        "        print(f\"  Size: {archive_size:.2f} GB\")\n",
        "        \n",
        "        # Auto-download\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(f\"\\n[DOWNLOAD] Downloading archive...\")\n",
        "            files.download(str(archive_path))\n",
        "            print(f\"[OK] Download started!\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[INFO] Auto-download failed: {e}\")\n",
        "            print(f\"   Run manually:\")\n",
        "            print(f\"   from google.colab import files\")\n",
        "            print(f\"   files.download('{archive_name}.zip')\")\n",
        "    else:\n",
        "        print(\"[WARNING] Models directory not found or empty.\")\n",
        "        print(f\"  Expected: {models_dir}\")\n",
        "else:\n",
        "    print(\"Not running in Colab - use Cell 28 for local archiving\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to download\n",
        "# from google.colab import files\n",
        "# files.download('trained_models.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU cache cleared\")\n",
        "\n",
        "# Optionally delete models to free up space\n",
        "# import shutil\n",
        "# shutil.rmtree(\"270FT/models\", ignore_errors=True)\n",
        "# print(\"Models directory deleted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "### Colab Runtime Information\n",
        "- **Free Tier**: T4 GPU (~16GB VRAM), ~12GB RAM, ~12 hour sessions\n",
        "- **Pro Tier**: V100 (16GB) or A100 (40GB) GPUs, up to 52GB RAM, ~24 hour sessions\n",
        "- **Current Config**: Optimized for T4 (16GB VRAM) with batch_size=2\n",
        "\n",
        "### Memory Requirements\n",
        "- **Training**: ~8-10GB VRAM with current settings (batch_size=2, gradient checkpointing)\n",
        "- **Inference**: ~4-5GB VRAM with 4-bit quantization\n",
        "- **Training Time**: Expect 3-6 hours per model on Colab T4\n",
        "\n",
        "### Configuration\n",
        "- **Batch Size**: 2 (with gradient accumulation of 2 for effective batch size of 4)\n",
        "- **Max Length**: 1024 tokens (can increase to 2048 on A100)\n",
        "- **Gradient Checkpointing**: Enabled (trades ~20% compute for memory)\n",
        "- **Quantization**: 4-bit NF4 for both training and inference\n",
        "\n",
        "### Colab-Specific Tips\n",
        "\n",
        "1. **Session Management**:\n",
        "   - Colab sessions disconnect after ~90 min of inactivity\n",
        "   - Save checkpoints frequently (configured to save every 500 steps)\n",
        "   - Use Google Drive to persist models (see Cell 29)\n",
        "   - Consider Colab Pro for longer sessions and better GPUs\n",
        "\n",
        "2. **Out of Memory (OOM) Errors**:\n",
        "   - Reduce `batch_size` to 1 in `training_config.yaml`\n",
        "   - Reduce `max_length` to 512\n",
        "   - Request High-RAM runtime: Runtime → Change runtime type → High-RAM\n",
        "   - Close other Colab tabs using GPU\n",
        "\n",
        "3. **Repository Setup**:\n",
        "   - **IMPORTANT**: Update `REPO_URL` in Cell 3 with your GitHub repository\n",
        "   - Or upload your project files directly to Colab\n",
        "\n",
        "4. **Data Upload**:\n",
        "   - Upload data files using Colab's file browser (left sidebar)\n",
        "   - Or use: `from google.colab import files; files.upload()`\n",
        "   - Or mount Google Drive and copy from there\n",
        "\n",
        "5. **Model Persistence**:\n",
        "   - Colab files are deleted when session ends\n",
        "   - Always download models or save to Google Drive (Cell 28-29)\n",
        "   - Models are ~50-100MB (LoRA adapters only)\n",
        "\n",
        "6. **Training Interrupted**:\n",
        "   - Checkpoints are saved every 500 steps\n",
        "   - Training will resume from last checkpoint if you restart\n",
        "   - Or load adapter and continue training manually\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "1. **Out of Memory (OOM) Errors**:\n",
        "   - Reduce `max_length` in `training_config.yaml` (try 512)\n",
        "   - Reduce `gradient_accumulation_steps` (try 2 instead of 4)\n",
        "   - Ensure no other processes are using GPU memory\n",
        "   - Close other notebooks/applications using GPU\n",
        "\n",
        "2. **Model Not Found**:\n",
        "   - Ensure training completed successfully (check Cell 16)\n",
        "   - Verify adapter files exist in `models/` directory\n",
        "\n",
        "3. **Import Errors**:\n",
        "   - Restart kernel and re-run setup cells (Cells 1-5)\n",
        "   - Ensure all dependencies are installed (Cell 4)\n",
        "\n",
        "4. **Training Fails**:\n",
        "   - Check that training data exists in `data/processed/train.jsonl` or `data/raw/train.json`\n",
        "   - Verify data format is correct (see README for format specifications)\n",
        "   - Check GPU memory before training (Cell 2)\n",
        "\n",
        "5. **Slow Training**:\n",
        "   - This is expected with 8GB VRAM due to memory optimizations\n",
        "   - Gradient checkpointing adds ~20-30% overhead but enables training on limited memory\n",
        "   - Consider using a cloud GPU with more VRAM for faster training\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with different LoRA hyperparameters (rank, alpha)\n",
        "- Try different base models (Qwen 2.5 1.5B for even lower memory usage)\n",
        "- Add more training data for better performance\n",
        "- Fine-tune the evaluation metrics\n",
        "- Use the CLI (`python cli.py query`) for interactive queries\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
