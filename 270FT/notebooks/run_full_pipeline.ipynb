{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning LLMs: Complete Pipeline in Google Colab\n",
        "\n",
        "This notebook runs the complete fine-tuning and evaluation pipeline for fine-tuning LLMs.\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Setup**: Clone repository and install dependencies\n",
        "2. **Data Preparation**: Load and prepare training data\n",
        "3. **Training**: Fine-tune LLaMA 3 and Qwen 3 with QLoRA\n",
        "4. **Evaluation**: Evaluate models on test set\n",
        "5. **Interactive Demo**: Use the CLI to query models\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "# Replace with your repository URL\n",
        "REPO_URL = \"https://github.com/yourusername/270FT.git\"  # Update this!\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Clone the repository\n",
        "if not os.path.exists(\"270FT\"):\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL], check=True)\n",
        "    print(\"[OK] Repository cloned\")\n",
        "else:\n",
        "    print(\"Repository already exists, skipping clone\")\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(\"270FT\")\n",
        "print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"-r\", \"requirements.txt\"], check=True)\n",
        "print(\"[OK] Dependencies installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "try:\n",
        "    import transformers\n",
        "    import peft\n",
        "    import sympy\n",
        "    import z3\n",
        "    import wandb\n",
        "    import yaml\n",
        "    print(\"[OK] All required packages installed successfully\")\n",
        "    print(f\"  - Transformers: {transformers.__version__}\")\n",
        "    print(f\"  - PEFT: {peft.__version__}\")\n",
        "    print(f\"  - SymPy: {sympy.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"[ERROR] Import error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directories if they don't exist\n",
        "from pathlib import Path\n",
        "\n",
        "data_raw = Path(\"270FT/data/raw\")\n",
        "data_processed = Path(\"270FT/data/processed\")\n",
        "\n",
        "data_raw.mkdir(parents=True, exist_ok=True)\n",
        "data_processed.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data directories created:\")\n",
        "print(f\"  - Raw: {data_raw}\")\n",
        "print(f\"  - Processed: {data_processed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create sample training data if it doesn't exist\n",
        "# In practice, you would upload your own data files\n",
        "\n",
        "import json\n",
        "\n",
        "sample_train_data = [\n",
        "    {\n",
        "        \"prompt\": \"Prove that the sum of the first n natural numbers is n(n+1)/2\",\n",
        "        \"response\": \"[Algorithm Outline]\\nUse mathematical induction to prove the formula.\\n\\n[Pseudocode]\\nfunction verify_sum(n):\\n    if n == 1:\\n        return 1 == 1 * 2 / 2  // Base case\\n    // Inductive step: assume true for k, prove for k+1\\n\\n[Proof Summary]\\nBase case (n=1): Sum = 1, formula = 1(2)/2 = 1\\nInductive step: Assume sum(1..k) = k(k+1)/2.\\nFor k+1: sum(1..k+1) = k(k+1)/2 + (k+1) = (k+1)(k+2)/2\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Explain the binary search algorithm\",\n",
        "        \"response\": \"[Algorithm Outline]\\nBinary search finds an element in a sorted array by repeatedly dividing the search space in half.\\n\\n[Pseudocode]\\nfunction binary_search(arr, target):\\n    left = 0\\n    right = len(arr) - 1\\n    while left <= right:\\n        mid = (left + right) // 2\\n        if arr[mid] == target:\\n            return mid\\n        elif arr[mid] < target:\\n            left = mid + 1\\n        else:\\n            right = mid - 1\\n    return -1\\n\\n[Proof Summary]\\nTime complexity: O(log n) because we halve the search space each iteration.\\nSpace complexity: O(1) for iterative version.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "sample_test_data = [\n",
        "    {\n",
        "        \"prompt\": \"Prove that 1 + 2 + ... + n = n(n+1)/2\",\n",
        "        \"response\": \"[Algorithm Outline]\\nMathematical induction proof.\\n\\n[Pseudocode]\\nBase: n=1 â†’ 1 = 1(2)/2 = 1\\nInductive: sum(1..k+1) = sum(1..k) + (k+1) = k(k+1)/2 + (k+1) = (k+1)(k+2)/2\\n\\n[Proof Summary]\\nBy mathematical induction, the formula holds for all natural numbers n.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save sample data (only if files don't exist)\n",
        "train_path = data_raw / \"train.json\"\n",
        "test_path = data_raw / \"test.json\"\n",
        "\n",
        "if not train_path.exists():\n",
        "    with open(train_path, \"w\") as f:\n",
        "        json.dump(sample_train_data, f, indent=2)\n",
        "    print(f\"[OK] Created sample training data: {train_path}\")\n",
        "else:\n",
        "    print(f\"Training data already exists: {train_path}\")\n",
        "\n",
        "if not test_path.exists():\n",
        "    with open(test_path, \"w\") as f:\n",
        "        json.dump(sample_test_data, f, indent=2)\n",
        "    print(f\"[OK] Created sample test data: {test_path}\")\n",
        "else:\n",
        "    print(f\"Test data already exists: {test_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data statistics\n",
        "if train_path.exists():\n",
        "    with open(train_path, \"r\") as f:\n",
        "        train_data = json.load(f)\n",
        "    print(f\"Training samples: {len(train_data)}\")\n",
        "    if train_data:\n",
        "        print(f\"Sample prompt: {train_data[0]['prompt'][:100]}...\")\n",
        "\n",
        "if test_path.exists():\n",
        "    with open(test_path, \"r\") as f:\n",
        "        test_data = json.load(f)\n",
        "    print(f\"Test samples: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display current training configuration\n",
        "import yaml\n",
        "\n",
        "config_path = Path(\"270FT/configs/training_config.yaml\")\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Models to train: {[m['name'] for m in config['models']]}\")\n",
        "print(f\"  Epochs: {config['training']['epochs']}\")\n",
        "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"  LoRA rank: {config['training']['lora_r']}\")\n",
        "print(f\"  LoRA alpha: {config['training']['lora_alpha']}\")\n",
        "print(f\"  LoRA dropout: {config['training']['lora_dropout']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Configure W&B for experiment tracking\n",
        "# Uncomment and run if you want to use Weights & Biases\n",
        "\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "# print(\"[OK] W&B configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training script\n",
        "# This will train both LLaMA 3 and Qwen 3 models\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"270FT\")\n",
        "\n",
        "print(\"Starting training pipeline...\")\n",
        "print(\"This may take several hours depending on your GPU and dataset size.\")\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute training\n",
        "# Note: In Colab, you can run this as a Python script\n",
        "\n",
        "import importlib.util\n",
        "spec = importlib.util.spec_from_file_location(\"train_dual_lora\", \"270FT/training/train_dual_lora.py\")\n",
        "train_dual_lora = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(train_dual_lora)\n",
        "train_main = train_dual_lora.main\n",
        "\n",
        "# Run training\n",
        "train_main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify models were saved\n",
        "models_dir = Path(\"270FT/models\")\n",
        "\n",
        "for model_config in config[\"models\"]:\n",
        "    model_path = models_dir / model_config[\"output_dir\"]\n",
        "    if model_path.exists():\n",
        "        files = list(model_path.iterdir())\n",
        "        print(f\"[OK] {model_config['name']} saved to {model_path}\")\n",
        "        print(f\"  Files: {[f.name for f in files[:5]]}...\")\n",
        "    else:\n",
        "        print(f\"[ERROR] {model_config['name']} not found at {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation script\n",
        "print(\"Running evaluation on test set...\")\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib.util\n",
        "spec = importlib.util.spec_from_file_location(\"evaluate_models\", \"270FT/evaluation/evaluate_models.py\")\n",
        "evaluate_models = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(evaluate_models)\n",
        "eval_main = evaluate_models.main\n",
        "\n",
        "# Run evaluation\n",
        "eval_main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display evaluation results\n",
        "results_path = Path(\"270FT/results/metrics_report.json\")\n",
        "\n",
        "if results_path.exists():\n",
        "    with open(results_path, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"\\nEvaluation Results Summary:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for model_name, model_results in results[\"model_results\"].items():\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(f\"  Exact Match Rate: {model_results['exact_match_rate']:.4f}\")\n",
        "        print(f\"  Symbolic Equivalence Rate: {model_results['symbolic_equivalence_rate']:.4f}\")\n",
        "        print(f\"  Average BLEU Score: {model_results['avg_bleu_score']:.4f}\")\n",
        "else:\n",
        "    print(\"Results file not found. Please run evaluation first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Interactive Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a model and generate a response\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from pathlib import Path\n",
        "\n",
        "def load_model_for_demo(model_name, adapter_path, device=\"cuda\"):\n",
        "    \"\"\"Load model with adapter for interactive use.\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\" if device == \"cuda\" else None,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    \n",
        "    model = PeftModel.from_pretrained(model, str(adapter_path))\n",
        "    model.eval()\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    print(f\"[OK] Model loaded\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(model, tokenizer, question, max_new_tokens=512):\n",
        "    \"\"\"Generate response to a question.\"\"\"\n",
        "    prompt = f\"### Question:\\n{question}\\n\\n### Solution:\\n\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    generated = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    return generated.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load first available model\n",
        "models_dir = Path(\"270FT/models\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Try to load the first model from config\n",
        "model_loaded = False\n",
        "for model_config in config[\"models\"]:\n",
        "    adapter_path = models_dir / model_config[\"output_dir\"]\n",
        "    \n",
        "    if adapter_path.exists() and (adapter_path / \"adapter_config.json\").exists():\n",
        "        try:\n",
        "            model, tokenizer = load_model_for_demo(\n",
        "                model_config[\"name\"],\n",
        "                adapter_path,\n",
        "                device=device\n",
        "            )\n",
        "            model_loaded = True\n",
        "            current_model_name = model_config[\"name\"]\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_config['name']}: {e}\")\n",
        "            continue\n",
        "\n",
        "if not model_loaded:\n",
        "    print(\"No trained models found. Please run training first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with a sample question\n",
        "if model_loaded:\n",
        "    test_question = \"Prove that the sum of the first n natural numbers is n(n+1)/2\"\n",
        "    \n",
        "    print(f\"Question: {test_question}\\n\")\n",
        "    print(\"Generating response...\\n\")\n",
        "    \n",
        "    response = generate_response(model, tokenizer, test_question)\n",
        "    \n",
        "    print(\"Response:\")\n",
        "    print(\"=\"*60)\n",
        "    print(response)\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive Query Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive cell - modify the question and run\n",
        "if model_loaded:\n",
        "    # Change this question to test different queries\n",
        "    your_question = \"Explain the quicksort algorithm\"\n",
        "    \n",
        "    print(f\"Question: {your_question}\\n\")\n",
        "    print(\"Generating response...\\n\")\n",
        "    \n",
        "    response = generate_response(model, tokenizer, your_question, max_new_tokens=1024)\n",
        "    \n",
        "    print(\"Response:\")\n",
        "    print(\"=\"*60)\n",
        "    print(response)\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"Please load a model first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Download Models (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compress and download trained models\n",
        "# This is useful if you want to save your trained models\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Create a zip file of the models directory\n",
        "models_dir = Path(\"270FT/models\")\n",
        "if models_dir.exists():\n",
        "    print(\"Creating archive of trained models...\")\n",
        "    shutil.make_archive(\"trained_models\", \"zip\", models_dir)\n",
        "    print(\"[OK] Archive created: trained_models.zip\")\n",
        "    print(\"\\nTo download, run:\")\n",
        "    print(\"  from google.colab import files\")\n",
        "    print(\"  files.download('trained_models.zip')\")\n",
        "else:\n",
        "    print(\"Models directory not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to download\n",
        "# from google.colab import files\n",
        "# files.download('trained_models.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU cache cleared\")\n",
        "\n",
        "# Optionally delete models to free up space\n",
        "# import shutil\n",
        "# shutil.rmtree(\"270FT/models\", ignore_errors=True)\n",
        "# print(\"Models directory deleted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "- **Training Time**: Expect 3-5 hours per model on Colab's free GPU (T4)\n",
        "- **Memory**: Models require ~15-18GB VRAM. Use Colab Pro for better GPUs if needed.\n",
        "- **Data**: Upload your own training data to `270FT/data/raw/` before training\n",
        "- **Persistence**: Colab sessions may disconnect. Consider saving checkpoints or using Colab Pro.\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "1. **Out of Memory**: Reduce batch size in `training_config.yaml`\n",
        "2. **Model Not Found**: Ensure you've cloned the repository correctly\n",
        "3. **Import Errors**: Restart runtime and re-run setup cells\n",
        "4. **Training Fails**: Check that training data exists in `270FT/data/raw/train.json`\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Experiment with different LoRA hyperparameters\n",
        "- Try different base models\n",
        "- Add more training data\n",
        "- Fine-tune the evaluation metrics\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
