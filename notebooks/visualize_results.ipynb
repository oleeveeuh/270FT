{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning LLMs: Results Visualization Notebook\n",
        "\n",
        "This notebook visualizes training results, model comparisons, and evaluation metrics for the fine-tuning LLM pipeline.\n",
        "\n",
        "## Prerequisites\n",
        "- Models must be trained first (run `run_full_pipeline.ipynb` or `train_dual_lora.py`)\n",
        "- Test data should be in `data/processed/test.json`\n",
        "- Evaluation results should be in `results/metrics_report.json` (automated metrics)\n",
        "- Human evaluation form should be in `results/human_evaluation_form.json` (optional, for human-in-the-loop evaluation)\n",
        "\n",
        "## Features\n",
        "- **Automated Metrics**: BLEU, Exact Match, Symbolic Equivalence\n",
        "- **Human Evaluation**: Mathematical Correctness, Completeness, Clarity, Overall Quality (1-5 scale)\n",
        "- **Side-by-Side Comparison**: Compare automated vs human evaluations\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate peft bitsandbytes sympy wandb matplotlib seaborn plotly networkx pandas\n",
        "!pip install -q datasets evaluate scikit-learn\n",
        "\n",
        "import os\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "print(\"[OK] Environment ready. Please connect GPU (Runtime → Change runtime type → GPU).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Models and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import yaml\n",
        "\n",
        "with open(\"configs/training_config.yaml\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "def load_model(model_name, adapter_dir):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, load_in_4bit=True, device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "llama_tok, llama_model = load_model(cfg[\"models\"][0][\"name\"], cfg[\"models\"][0][\"output_dir\"])\n",
        "qwen_tok, qwen_model = load_model(cfg[\"models\"][1][\"name\"], cfg[\"models\"][1][\"output_dir\"])\n",
        "\n",
        "print(\"[OK] Models loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Test Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"data/processed/test.json\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test problems.\")\n",
        "print(test_data[0].keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Predictions and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "def generate_answer(model, tokenizer, question):\n",
        "    input_text = f\"### Question:\\n{question}\\n### Solution:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=512)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"### Solution:\")[-1].strip()\n",
        "\n",
        "results = []\n",
        "for item in tqdm(test_data):\n",
        "    q = item[\"question\"]\n",
        "    gt = item[\"answer\"]\n",
        "    llama_pred = generate_answer(llama_model, llama_tok, q)\n",
        "    qwen_pred = generate_answer(qwen_model, qwen_tok, q)\n",
        "    results.append({\n",
        "        \"question\": q,\n",
        "        \"ground_truth\": gt,\n",
        "        \"llama_pred\": llama_pred,\n",
        "        \"qwen_pred\": qwen_pred\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"results/raw_predictions.csv\", index=False)\n",
        "print(\"[OK] Predictions generated and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compute Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from evaluate import load as load_metric\n",
        "from sympy import simplify, symbols, parse_expr, Eq\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def compute_exact_match(ref, pred):\n",
        "    \"\"\"Compute exact match score.\"\"\"\n",
        "    return ref.strip().lower() == pred.strip().lower()\n",
        "\n",
        "def compute_bleu(ref, pred):\n",
        "    \"\"\"Compute BLEU score.\"\"\"\n",
        "    try:\n",
        "        bleu_metric = load_metric(\"bleu\")\n",
        "        references = [[ref.split()]]\n",
        "        predictions = [pred.split()]\n",
        "        result = bleu_metric.compute(predictions=predictions, references=references)\n",
        "        return result.get(\"bleu\", 0.0)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def check_symbolic_equivalence(ref, pred):\n",
        "    \"\"\"Check symbolic equivalence using SymPy.\"\"\"\n",
        "    try:\n",
        "        # Extract expressions (simplified version)\n",
        "        x, n = symbols('x n')\n",
        "        ref_clean = re.sub(r'[^a-zA-Z0-9\\s\\+\\-\\*/\\^\\(\\)=]', '', ref)\n",
        "        pred_clean = re.sub(r'[^a-zA-Z0-9\\s\\+\\-\\*/\\^\\(\\)=]', '', pred)\n",
        "        \n",
        "        try:\n",
        "            ref_expr = parse_expr(ref_clean, transformations='all')\n",
        "            pred_expr = parse_expr(pred_clean, transformations='all')\n",
        "            diff = simplify(ref_expr - pred_expr)\n",
        "            return diff == 0\n",
        "        except:\n",
        "            return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Compute metrics for both models using the results DataFrame\n",
        "llama_metrics = {\n",
        "    \"exact_match\": [],\n",
        "    \"bleu\": [],\n",
        "    \"symbolic_equiv\": []\n",
        "}\n",
        "\n",
        "qwen_metrics = {\n",
        "    \"exact_match\": [],\n",
        "    \"bleu\": [],\n",
        "    \"symbolic_equiv\": []\n",
        "}\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    reference = row[\"ground_truth\"]\n",
        "    llama_pred = row[\"llama_pred\"]\n",
        "    qwen_pred = row[\"qwen_pred\"]\n",
        "    \n",
        "    # LLaMA metrics\n",
        "    llama_metrics[\"exact_match\"].append(compute_exact_match(reference, llama_pred))\n",
        "    llama_metrics[\"bleu\"].append(compute_bleu(reference, llama_pred))\n",
        "    llama_metrics[\"symbolic_equiv\"].append(check_symbolic_equivalence(reference, llama_pred))\n",
        "    \n",
        "    # Qwen metrics\n",
        "    qwen_metrics[\"exact_match\"].append(compute_exact_match(reference, qwen_pred))\n",
        "    qwen_metrics[\"bleu\"].append(compute_bleu(reference, qwen_pred))\n",
        "    qwen_metrics[\"symbolic_equiv\"].append(check_symbolic_equivalence(reference, qwen_pred))\n",
        "\n",
        "# Add metrics to DataFrame\n",
        "df[\"llama_exact_match\"] = llama_metrics[\"exact_match\"]\n",
        "df[\"llama_bleu\"] = llama_metrics[\"bleu\"]\n",
        "df[\"llama_symbolic_equiv\"] = llama_metrics[\"symbolic_equiv\"]\n",
        "df[\"qwen_exact_match\"] = qwen_metrics[\"exact_match\"]\n",
        "df[\"qwen_bleu\"] = qwen_metrics[\"bleu\"]\n",
        "df[\"qwen_symbolic_equiv\"] = qwen_metrics[\"symbolic_equiv\"]\n",
        "\n",
        "print(\"[OK] Metrics computed and added to DataFrame.\")\n",
        "print(f\"\\nSample metrics:\")\n",
        "print(df[[\"question\", \"llama_exact_match\", \"llama_bleu\", \"qwen_exact_match\", \"qwen_bleu\"]].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Symbolic Equivalence Check with SymPy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sympy import simplify, sympify\n",
        "\n",
        "def check_symbolic_equivalence(gt, pred):\n",
        "    try:\n",
        "        return simplify(sympify(gt) - sympify(pred)) == 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "df[\"llama_symbolic\"] = df.apply(lambda r: check_symbolic_equivalence(r[\"ground_truth\"], r[\"llama_pred\"]), axis=1)\n",
        "df[\"qwen_symbolic\"] = df.apply(lambda r: check_symbolic_equivalence(r[\"ground_truth\"], r[\"qwen_pred\"]), axis=1)\n",
        "\n",
        "print(df[[\"llama_symbolic\", \"qwen_symbolic\"]].mean())\n",
        "df.to_csv(\"results/symbolic_results.csv\", index=False)\n",
        "print(\"\\n[OK] Symbolic equivalence checked and saved to results/symbolic_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Compute Aggregate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "bleu = load_metric(\"bleu\")\n",
        "\n",
        "def compute_metrics(df, model_col):\n",
        "    exact = (df[model_col].str.strip() == df[\"ground_truth\"].str.strip()).mean()\n",
        "    bleu_score = bleu.compute(predictions=[[p.split()] for p in df[model_col]], references=[[[r.split()]] for r in df[\"ground_truth\"]])[\"bleu\"]\n",
        "    symbolic = df[f\"{model_col.split('_')[0]}_symbolic\"].mean()\n",
        "    return {\"Exact Match\": exact, \"BLEU\": bleu_score, \"Symbolic Eq\": symbolic}\n",
        "\n",
        "llama_metrics = compute_metrics(df, \"llama_pred\")\n",
        "qwen_metrics = compute_metrics(df, \"qwen_pred\")\n",
        "\n",
        "metrics_df = pd.DataFrame([llama_metrics, qwen_metrics], index=[\"LLaMA3\", \"Qwen3\"])\n",
        "metrics_df.to_csv(\"results/metrics_summary.csv\")\n",
        "print(\"[OK] Metrics summary saved to results/metrics_summary.csv\")\n",
        "print(\"\\nAggregate Metrics:\")\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Comprehensive Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Metric Comparison ---\n",
        "metrics_df.plot(kind=\"bar\", figsize=(8,5), title=\"Model Performance Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/metric_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# --- Symbolic Verification Pie Charts ---\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
        "for i, model in enumerate([\"llama_symbolic\", \"qwen_symbolic\"]):\n",
        "    vals = [df[model].sum(), len(df) - df[model].sum()]\n",
        "    ax[i].pie(vals, labels=[\"Correct\",\"Incorrect\"], autopct=\"%1.1f%%\", colors=[\"#4CAF50\",\"#F44336\"])\n",
        "    ax[i].set_title(model.replace(\"_symbolic\",\"\").upper())\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/symbolic_pies.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# --- Qualitative Case Study Table ---\n",
        "sample_df = df.sample(3)\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\"### Sample Comparison Table\"))\n",
        "display(sample_df[[\"question\",\"ground_truth\",\"llama_pred\",\"qwen_pred\"]])\n",
        "\n",
        "print(\"[OK] Visualizations generated and saved in /figures.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Model Size vs Symbolic Accuracy Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eff_df = pd.DataFrame({\n",
        "    \"Model\": [\"LLaMA3\",\"Qwen3\"],\n",
        "    \"Params (B)\": [8, 7],\n",
        "    \"VRAM (GB)\": [18, 16],\n",
        "    \"Symbolic Accuracy\": [llama_metrics[\"Symbolic Eq\"], qwen_metrics[\"Symbolic Eq\"]],\n",
        "    \"Training Time (min)\": [92, 84]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(eff_df[\"VRAM (GB)\"], eff_df[\"Symbolic Accuracy\"], s=eff_df[\"Training Time (min)\"]*2, alpha=0.7)\n",
        "for i,row in eff_df.iterrows():\n",
        "    plt.text(row[\"VRAM (GB)\"]+0.2, row[\"Symbolic Accuracy\"], row[\"Model\"])\n",
        "plt.xlabel(\"VRAM Usage (GB)\")\n",
        "plt.ylabel(\"Symbolic Accuracy\")\n",
        "plt.title(\"Efficiency Tradeoff\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/efficiency_plot.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"[OK] Efficiency plot saved to figures/efficiency_plot.png\")\n",
        "print(\"\\nEfficiency Summary:\")\n",
        "print(eff_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Human-in-the-Loop Evaluation\n",
        "\n",
        "This section compares automated metrics with human expert evaluations (professor ratings).\n",
        "\n",
        "### 7.1 Load Human Evaluation Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load human evaluation form (if available)\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "human_eval_path = Path(\"results/human_evaluation_form.json\")\n",
        "human_data = None\n",
        "\n",
        "if human_eval_path.exists():\n",
        "    with open(human_eval_path, 'r') as f:\n",
        "        human_data = json.load(f)\n",
        "    \n",
        "    # Extract human scores\n",
        "    human_scores = {\n",
        "        \"mathematical_correctness\": [],\n",
        "        \"completeness\": [],\n",
        "        \"clarity\": [],\n",
        "        \"overall_quality\": []\n",
        "    }\n",
        "    \n",
        "    completed = 0\n",
        "    for item in human_data.get(\"items\", []):\n",
        "        eval_data = item.get(\"human_evaluation\", {})\n",
        "        if eval_data.get(\"overall_quality\") is not None:\n",
        "            completed += 1\n",
        "            for criterion in human_scores.keys():\n",
        "                score = eval_data.get(criterion)\n",
        "                if score is not None:\n",
        "                    human_scores[criterion].append(score)\n",
        "    \n",
        "    print(f\"Human evaluation loaded: {completed}/{len(human_data.get('items', []))} items completed\")\n",
        "    print(f\"Average scores:\")\n",
        "    for criterion, scores in human_scores.items():\n",
        "        if scores:\n",
        "            print(f\"  {criterion}: {np.mean(scores):.2f} (scale: 1-5)\")\n",
        "else:\n",
        "    print(\"Human evaluation form not found. Run:\")\n",
        "    print(\"  python -m 270FT.evaluation.human_evaluation --test_results results/metrics_report.json --output results/human_evaluation_form.json\")\n",
        "    print(\"\\nThen fill in the scores and re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Side-by-Side Comparison: Automated vs Human Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if human_data:\n",
        "    # Prepare data for side-by-side comparison\n",
        "    # Normalize automated metrics to 0-5 scale for comparison\n",
        "    # BLEU: 0-1 -> 0-5 (multiply by 5)\n",
        "    # Exact Match: 0-1 -> 0-5 (multiply by 5)\n",
        "    # Symbolic Equivalence: 0-1 -> 0-5 (multiply by 5)\n",
        "    \n",
        "    # Get automated metrics from first model (assuming similar across models)\n",
        "    auto_metrics_normalized = {\n",
        "        \"Exact Match\": np.mean(llama_metrics[\"exact_match\"]) * 5,\n",
        "        \"BLEU Score\": np.mean(llama_metrics[\"bleu\"]) * 5,\n",
        "        \"Symbolic Equivalence\": np.mean(llama_metrics[\"symbolic_equiv\"]) * 5\n",
        "    }\n",
        "    \n",
        "    # Get human metrics (already on 1-5 scale)\n",
        "    human_metrics_avg = {\n",
        "        \"Mathematical Correctness\": np.mean(human_scores[\"mathematical_correctness\"]) if human_scores[\"mathematical_correctness\"] else None,\n",
        "        \"Completeness\": np.mean(human_scores[\"completeness\"]) if human_scores[\"completeness\"] else None,\n",
        "        \"Clarity\": np.mean(human_scores[\"clarity\"]) if human_scores[\"clarity\"] else None,\n",
        "        \"Overall Quality\": np.mean(human_scores[\"overall_quality\"]) if human_scores[\"overall_quality\"] else None\n",
        "    }\n",
        "    \n",
        "    # Create side-by-side comparison plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Left: Automated Metrics (normalized to 0-5)\n",
        "    auto_categories = list(auto_metrics_normalized.keys())\n",
        "    auto_values = list(auto_metrics_normalized.values())\n",
        "    \n",
        "    bars1 = axes[0].bar(auto_categories, auto_values, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    axes[0].set_title('Automated Metrics (Normalized to 0-5 Scale)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Score (0-5)', fontsize=12)\n",
        "    axes[0].set_ylim(0, 5)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars1, auto_values):\n",
        "        height = bar.get_height()\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                    f'{val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # Right: Human Evaluation Metrics (1-5 scale)\n",
        "    human_categories = [k for k, v in human_metrics_avg.items() if v is not None]\n",
        "    human_values = [v for v in human_metrics_avg.values() if v is not None]\n",
        "    \n",
        "    bars2 = axes[1].bar(human_categories, human_values, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "    axes[1].set_title('Human Evaluation Metrics (1-5 Scale)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Score (1-5)', fontsize=12)\n",
        "    axes[1].set_ylim(0, 5)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars2, human_values):\n",
        "        height = bar.get_height()\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                    f'{val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"figures/automated_vs_human_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"[OK] Side-by-side comparison saved to figures/automated_vs_human_comparison.png\")\n",
        "    \n",
        "    # Create comparison table\n",
        "    comparison_df = pd.DataFrame({\n",
        "        \"Metric Type\": [\"Automated\"] * len(auto_categories) + [\"Human\"] * len(human_categories),\n",
        "        \"Metric\": auto_categories + human_categories,\n",
        "        \"Score\": auto_values + human_values,\n",
        "        \"Scale\": [\"0-5 (normalized)\"] * len(auto_categories) + [\"1-5\"] * len(human_categories)\n",
        "    })\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"AUTOMATED vs HUMAN METRICS COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    comparison_df.to_csv(\"results/automated_vs_human_comparison.csv\", index=False)\n",
        "    print(\"\\n[OK] Comparison table saved to results/automated_vs_human_comparison.csv\")\n",
        "else:\n",
        "    print(\"Human evaluation data not available. Please complete the evaluation form first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Correlation: Automated Metrics vs Human Scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if human_data and len(human_scores[\"overall_quality\"]) > 0:\n",
        "    # Match automated metrics with human scores per item\n",
        "    items = human_data.get(\"items\", [])\n",
        "    \n",
        "    # Extract per-item data\n",
        "    item_data = []\n",
        "    for item in items:\n",
        "        auto_metrics = item.get(\"automated_metrics\", {})\n",
        "        human_eval = item.get(\"human_evaluation\", {})\n",
        "        \n",
        "        if human_eval.get(\"overall_quality\") is not None:\n",
        "            item_data.append({\n",
        "                \"exact_match\": 1 if auto_metrics.get(\"exact_match\") else 0,\n",
        "                \"bleu_score\": auto_metrics.get(\"bleu_score\", 0.0),\n",
        "                \"symbolic_equiv\": 1 if auto_metrics.get(\"symbolic_equivalence\") else 0,\n",
        "                \"math_correctness\": human_eval.get(\"mathematical_correctness\"),\n",
        "                \"completeness\": human_eval.get(\"completeness\"),\n",
        "                \"clarity\": human_eval.get(\"clarity\"),\n",
        "                \"overall_quality\": human_eval.get(\"overall_quality\")\n",
        "            })\n",
        "    \n",
        "    if item_data:\n",
        "        corr_df = pd.DataFrame(item_data)\n",
        "        \n",
        "        # Compute correlation matrix\n",
        "        corr_matrix = corr_df.corr()\n",
        "        \n",
        "        # Plot correlation heatmap\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
        "                   vmin=-1, vmax=1, ax=ax, square=True, linewidths=0.5,\n",
        "                   cbar_kws={'label': 'Correlation Coefficient'})\n",
        "        ax.set_title('Correlation: Automated Metrics vs Human Evaluation', \n",
        "                    fontsize=14, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"figures/automated_human_correlation.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"[OK] Correlation heatmap saved to figures/automated_human_correlation.png\")\n",
        "        \n",
        "        # Print key correlations\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"KEY CORRELATIONS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"BLEU Score vs Overall Quality: {corr_matrix.loc['bleu_score', 'overall_quality']:.3f}\")\n",
        "        print(f\"Exact Match vs Overall Quality: {corr_matrix.loc['exact_match', 'overall_quality']:.3f}\")\n",
        "        print(f\"Symbolic Equiv vs Math Correctness: {corr_matrix.loc['symbolic_equiv', 'math_correctness']:.3f}\")\n",
        "        print(f\"BLEU Score vs Completeness: {corr_matrix.loc['bleu_score', 'completeness']:.3f}\")\n",
        "        print(\"=\"*60)\n",
        "else:\n",
        "    print(\"Human evaluation data not available for correlation analysis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Prepare data for visualization from the DataFrame\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"Model\": [\"LLaMA 3\"] * len(df) + [\"Qwen 3\"] * len(df),\n",
        "    \"Exact Match\": df[\"llama_exact_match\"].tolist() + df[\"qwen_exact_match\"].tolist(),\n",
        "    \"BLEU Score\": df[\"llama_bleu\"].tolist() + df[\"qwen_bleu\"].tolist(),\n",
        "    \"Symbolic Equivalence\": df[\"llama_symbolic_equiv\"].tolist() + df[\"qwen_symbolic_equiv\"].tolist()\n",
        "})\n",
        "\n",
        "print(\"DataFrame prepared for visualization:\")\n",
        "print(metrics_df.head())\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Model Comparison - Aggregate Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate aggregate metrics\n",
        "agg_metrics = pd.DataFrame({\n",
        "    \"Model\": [\"LLaMA 3\", \"Qwen 3\"],\n",
        "    \"Exact Match Rate\": [\n",
        "        np.mean(llama_metrics[\"exact_match\"]),\n",
        "        np.mean(qwen_metrics[\"exact_match\"])\n",
        "    ],\n",
        "    \"Avg BLEU Score\": [\n",
        "        np.mean(llama_metrics[\"bleu\"]),\n",
        "        np.mean(qwen_metrics[\"bleu\"])\n",
        "    ],\n",
        "    \"Symbolic Equivalence Rate\": [\n",
        "        np.mean(llama_metrics[\"symbolic_equiv\"]),\n",
        "        np.mean(qwen_metrics[\"symbolic_equiv\"])\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Create comparison bar plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics_to_plot = [\"Exact Match Rate\", \"Avg BLEU Score\", \"Symbolic Equivalence Rate\"]\n",
        "colors = [\"#3498db\", \"#e74c3c\"]\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    axes[idx].bar(agg_metrics[\"Model\"], agg_metrics[metric], color=colors)\n",
        "    axes[idx].set_title(metric, fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel(\"Score\", fontsize=10)\n",
        "    axes[idx].set_ylim(0, 1)\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(agg_metrics[metric]):\n",
        "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/model_comparison_metrics.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"[OK] Comparison plot saved to figures/model_comparison_metrics.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Distribution of Scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Exact Match distribution\n",
        "exact_match_data = pd.DataFrame({\n",
        "    \"LLaMA 3\": llama_metrics[\"exact_match\"],\n",
        "    \"Qwen 3\": qwen_metrics[\"exact_match\"]\n",
        "})\n",
        "exact_match_data.plot(kind='hist', bins=2, ax=axes[0], alpha=0.7, color=colors)\n",
        "axes[0].set_title(\"Exact Match Distribution\", fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel(\"Match (0=No, 1=Yes)\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].legend()\n",
        "\n",
        "# BLEU Score distribution\n",
        "bleu_data = pd.DataFrame({\n",
        "    \"LLaMA 3\": llama_metrics[\"bleu\"],\n",
        "    \"Qwen 3\": qwen_metrics[\"bleu\"]\n",
        "})\n",
        "bleu_data.plot(kind='hist', bins=20, ax=axes[1], alpha=0.7, color=colors)\n",
        "axes[1].set_title(\"BLEU Score Distribution\", fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel(\"BLEU Score\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "axes[1].legend()\n",
        "\n",
        "# Symbolic Equivalence distribution\n",
        "sym_data = pd.DataFrame({\n",
        "    \"LLaMA 3\": llama_metrics[\"symbolic_equiv\"],\n",
        "    \"Qwen 3\": qwen_metrics[\"symbolic_equiv\"]\n",
        "})\n",
        "sym_data.plot(kind='hist', bins=2, ax=axes[2], alpha=0.7, color=colors)\n",
        "axes[2].set_title(\"Symbolic Equivalence Distribution\", fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel(\"Equivalence (0=No, 1=Yes)\")\n",
        "axes[2].set_ylabel(\"Frequency\")\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/score_distributions.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"[OK] Distribution plots saved to figures/score_distributions.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Box Plots for Score Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# BLEU Score box plot\n",
        "bleu_df = pd.DataFrame({\n",
        "    \"LLaMA 3\": llama_metrics[\"bleu\"],\n",
        "    \"Qwen 3\": qwen_metrics[\"bleu\"]\n",
        "})\n",
        "bleu_df.boxplot(ax=axes[0], color=dict(boxes=colors[0], whiskers=colors[0], medians=colors[1]))\n",
        "axes[0].set_title(\"BLEU Score Comparison\", fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel(\"BLEU Score\")\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Exact Match (as percentage)\n",
        "exact_df = pd.DataFrame({\n",
        "    \"LLaMA 3\": [x * 100 for x in llama_metrics[\"exact_match\"]],\n",
        "    \"Qwen 3\": [x * 100 for x in qwen_metrics[\"exact_match\"]]\n",
        "})\n",
        "exact_df.boxplot(ax=axes[1], color=dict(boxes=colors[0], whiskers=colors[0], medians=colors[1]))\n",
        "axes[1].set_title(\"Exact Match Comparison\", fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel(\"Exact Match (%)\")\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Symbolic Equivalence (as percentage)\n",
        "sym_df = pd.DataFrame({\n",
        "    \"LLaMA 3\": [x * 100 for x in llama_metrics[\"symbolic_equiv\"]],\n",
        "    \"Qwen 3\": [x * 100 for x in qwen_metrics[\"symbolic_equiv\"]]\n",
        "})\n",
        "sym_df.boxplot(ax=axes[2], color=dict(boxes=colors[0], whiskers=colors[0], medians=colors[1]))\n",
        "axes[2].set_title(\"Symbolic Equivalence Comparison\", fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylabel(\"Symbolic Equivalence (%)\")\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/box_plots_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"[OK] Box plots saved to figures/box_plots_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix for LLaMA 3\n",
        "llama_corr = pd.DataFrame({\n",
        "    \"Exact Match\": llama_metrics[\"exact_match\"],\n",
        "    \"BLEU\": llama_metrics[\"bleu\"],\n",
        "    \"Symbolic Equiv\": llama_metrics[\"symbolic_equiv\"]\n",
        "}).corr()\n",
        "\n",
        "# Correlation matrix for Qwen 3\n",
        "qwen_corr = pd.DataFrame({\n",
        "    \"Exact Match\": qwen_metrics[\"exact_match\"],\n",
        "    \"BLEU\": qwen_metrics[\"bleu\"],\n",
        "    \"Symbolic Equiv\": qwen_metrics[\"symbolic_equiv\"]\n",
        "}).corr()\n",
        "\n",
        "# Plot correlation matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.heatmap(llama_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
        "            vmin=-1, vmax=1, ax=axes[0], cbar_kws={'label': 'Correlation'})\n",
        "axes[0].set_title(\"LLaMA 3 - Metric Correlations\", fontsize=12, fontweight='bold')\n",
        "\n",
        "sns.heatmap(qwen_corr, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
        "            vmin=-1, vmax=1, ax=axes[1], cbar_kws={'label': 'Correlation'})\n",
        "axes[1].set_title(\"Qwen 3 - Metric Correlations\", fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/correlation_matrices.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"[OK] Correlation matrices saved to figures/correlation_matrices.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Performance Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive summary table\n",
        "summary_data = {\n",
        "    \"Metric\": [\"Exact Match Rate\", \"Avg BLEU Score\", \"Symbolic Equivalence Rate\",\n",
        "               \"Std BLEU Score\", \"Min BLEU\", \"Max BLEU\"],\n",
        "    \"LLaMA 3\": [\n",
        "        f\"{np.mean(llama_metrics['exact_match']):.4f}\",\n",
        "        f\"{np.mean(llama_metrics['bleu']):.4f}\",\n",
        "        f\"{np.mean(llama_metrics['symbolic_equiv']):.4f}\",\n",
        "        f\"{np.std(llama_metrics['bleu']):.4f}\",\n",
        "        f\"{np.min(llama_metrics['bleu']):.4f}\",\n",
        "        f\"{np.max(llama_metrics['bleu']):.4f}\"\n",
        "    ],\n",
        "    \"Qwen 3\": [\n",
        "        f\"{np.mean(qwen_metrics['exact_match']):.4f}\",\n",
        "        f\"{np.mean(qwen_metrics['bleu']):.4f}\",\n",
        "        f\"{np.mean(qwen_metrics['symbolic_equiv']):.4f}\",\n",
        "        f\"{np.std(qwen_metrics['bleu']):.4f}\",\n",
        "        f\"{np.min(qwen_metrics['bleu']):.4f}\",\n",
        "        f\"{np.max(qwen_metrics['bleu']):.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save to CSV\n",
        "summary_df.to_csv(\"results/performance_summary.csv\", index=False)\n",
        "print(\"\\n[OK] Summary saved to results/performance_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.6 Interactive Plotly Visualizations (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    \n",
        "    # Create interactive comparison\n",
        "    fig = make_subplots(\n",
        "        rows=1, cols=3,\n",
        "        subplot_titles=(\"Exact Match Rate\", \"BLEU Score\", \"Symbolic Equivalence\"),\n",
        "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
        "    )\n",
        "    \n",
        "    models = [\"LLaMA 3\", \"Qwen 3\"]\n",
        "    exact_rates = [np.mean(llama_metrics[\"exact_match\"]), np.mean(qwen_metrics[\"exact_match\"])]\n",
        "    bleu_scores = [np.mean(llama_metrics[\"bleu\"]), np.mean(qwen_metrics[\"bleu\"])]\n",
        "    sym_rates = [np.mean(llama_metrics[\"symbolic_equiv\"]), np.mean(qwen_metrics[\"symbolic_equiv\"])]\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=models, y=exact_rates, name=\"Exact Match\", marker_color=colors),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=models, y=bleu_scores, name=\"BLEU\", marker_color=colors),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=models, y=sym_rates, name=\"Symbolic Equiv\", marker_color=colors),\n",
        "        row=1, col=3\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title_text=\"Interactive Model Comparison\",\n",
        "        showlegend=False,\n",
        "        height=400\n",
        "    )\n",
        "    \n",
        "    fig.update_yaxes(range=[0, 1], row=1, col=1)\n",
        "    fig.update_yaxes(range=[0, 1], row=1, col=2)\n",
        "    fig.update_yaxes(range=[0, 1], row=1, col=3)\n",
        "    \n",
        "    fig.write_html(\"figures/interactive_comparison.html\")\n",
        "    fig.show()\n",
        "    \n",
        "    print(\"[OK] Interactive plot saved to figures/interactive_comparison.html\")\n",
        "except ImportError:\n",
        "    print(\"Plotly not available, skipping interactive visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results to JSON\n",
        "detailed_results = {\n",
        "    \"summary\": {\n",
        "        \"llama3\": {\n",
        "            \"exact_match_rate\": float(np.mean(df[\"llama_exact_match\"])),\n",
        "            \"avg_bleu\": float(np.mean(df[\"llama_bleu\"])),\n",
        "            \"symbolic_equiv_rate\": float(np.mean(df[\"llama_symbolic_equiv\"]))\n",
        "        },\n",
        "        \"qwen3\": {\n",
        "            \"exact_match_rate\": float(np.mean(df[\"qwen_exact_match\"])),\n",
        "            \"avg_bleu\": float(np.mean(df[\"qwen_bleu\"])),\n",
        "            \"symbolic_equiv_rate\": float(np.mean(df[\"qwen_symbolic_equiv\"]))\n",
        "        }\n",
        "    },\n",
        "    \"per_item_results\": []\n",
        "}\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    detailed_results[\"per_item_results\"].append({\n",
        "        \"item_id\": int(idx),\n",
        "        \"question\": row[\"question\"],\n",
        "        \"ground_truth\": row[\"ground_truth\"],\n",
        "        \"llama_prediction\": row[\"llama_pred\"],\n",
        "        \"qwen_prediction\": row[\"qwen_pred\"],\n",
        "        \"llama_metrics\": {\n",
        "            \"exact_match\": bool(row[\"llama_exact_match\"]),\n",
        "            \"bleu\": float(row[\"llama_bleu\"]),\n",
        "            \"symbolic_equiv\": bool(row[\"llama_symbolic_equiv\"])\n",
        "        },\n",
        "        \"qwen_metrics\": {\n",
        "            \"exact_match\": bool(row[\"qwen_exact_match\"]),\n",
        "            \"bleu\": float(row[\"qwen_bleu\"]),\n",
        "            \"symbolic_equiv\": bool(row[\"qwen_symbolic_equiv\"])\n",
        "        }\n",
        "    })\n",
        "\n",
        "with open(\"results/detailed_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(detailed_results, f, indent=2)\n",
        "\n",
        "# Also save the full DataFrame with all metrics\n",
        "df.to_csv(\"results/predictions_with_metrics.csv\", index=False)\n",
        "\n",
        "print(\"[OK] Detailed results saved to results/detailed_evaluation_results.json\")\n",
        "print(\"[OK] Full DataFrame with metrics saved to results/predictions_with_metrics.csv\")\n",
        "print(f\"\\nTotal items evaluated: {len(df)}\")\n",
        "print(f\"Figures saved to: figures/\")\n",
        "print(f\"Results saved to: results/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook provides comprehensive visualization and analysis of model performance:\n",
        "\n",
        "1. **Model Comparison**: Side-by-side comparison of LLaMA 3 and Qwen 3\n",
        "2. **Score Distributions**: Understanding how metrics vary across test items\n",
        "3. **Statistical Analysis**: Box plots, correlations, and summary statistics\n",
        "4. **Interactive Visualizations**: Plotly charts for exploration\n",
        "5. **Export**: All results saved to `results/` and `figures/` directories\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "- Compare exact match rates between models\n",
        "- Analyze BLEU score distributions\n",
        "- Check symbolic equivalence performance\n",
        "- Identify correlation patterns between metrics\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Analyze specific failure cases\n",
        "- Compare predictions side-by-side\n",
        "- Fine-tune models based on insights\n",
        "- Expand test dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
