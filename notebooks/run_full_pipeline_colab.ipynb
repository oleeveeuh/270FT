{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) (Optional) Mount Google Drive to persist checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Clone the repository (or pull if already present)\n",
    "import os\n",
    "if not os.path.exists('270FT'):\n",
    "    !git clone https://github.com/oleeveeuh/270FT.git\n",
    "%cd 270FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Install required Python packages (may take a few minutes).\n",
    "# bitsandbytes requires a CUDA-compatible runtime; if installation fails, try a different Colab runtime or match a wheel to the CUDA version.\n",
    "!pip install -q transformers datasets peft evaluate pyyaml huggingface_hub wandb bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d34f81",
   "metadata": {},
   "source": [
    "If bitsandbytes or other installs fail due to CUDA mismatches, try switching the Colab runtime GPU type (or use a notebook with a supported CUDA version). You can also skip installing `bitsandbytes` if you only want CPU runs, but the full QLoRA workflow requires a CUDA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171da73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) (Optional) Preprocess raw data into processed JSONL files.\n",
    "# This will create/update data/processed/train.jsonl, validation.jsonl, and test.jsonl\n",
    "!python preprocess/load_and_prepare.py --raw_dir data/raw --processed_dir data/processed --validation_split 0.15 --test_split 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) (Optional) Inspect processed files sizes and line counts\n",
    "!ls -lh data/processed || true\n",
    "!wc -l data/processed/*.jsonl || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e16ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Provide Hugging Face token (if required).\n",
    "# Use this cell to securely input a token if you need to access gated models.\n",
    "from getpass import getpass\n",
    "token = getpass('Hugging Face token (leave blank if not needed): ')\n",
    "import os\n",
    "if token:\n",
    "    os.environ['HF_TOKEN'] = token\n",
    "    print('HF_TOKEN set in environment')\n",
    "else:\n",
    "    print('No token provided; proceeding without HF token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b) Disable W&B interactive logging to avoid login prompts during automated runs\n",
    "import os\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "print('W&B offline mode set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eaf17e",
   "metadata": {},
   "source": [
    "Now run the full training script below. The script `training/train_dual_lora.py` will look for `data/processed/train.jsonl`, `validation.jsonl`, and `test.jsonl` in `data/processed` and will use the models listed in `configs/training_config.yaml`. Ensure those files are present (see the preprocessing step above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Run the training script (this will start QLoRA fine-tuning).\n",
    "# Note: training will log to W&B if enabled in the config; consider disabling W&B or logging in first.\n",
    "!python training/train_dual_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2214c4f",
   "metadata": {},
   "source": [
    "Troubleshooting tips:\n",
    "- If you see `FileNotFoundError` complaining about missing `test.jsonl`, re-run the preprocessing cell.\n",
    "- If you get `bitsandbytes` import errors, try installing a different `bitsandbytes` wheel that matches the Colab CUDA runtime or switch runtime.\n",
    "- If training runs out of VRAM, reduce `batch_size` in `configs/training_config.yaml` or switch to a larger GPU.\n",
    "\n",
    "Next steps:\n",
    "- If you want, I can generate a copy of this notebook as an `.ipynb` file in the repo (already created) and also add `run_colab.sh` or config presets optimized for T4/A100."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
