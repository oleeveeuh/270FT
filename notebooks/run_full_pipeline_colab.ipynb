{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) (Optional) Mount Google Drive to persist checkpoints\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except Exception as e:\n",
    "    print('Not running in Colab or drive mount failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Clone the repository (or pull if already present)\n",
    "import os, subprocess, sys\n",
    "if not os.path.exists('270FT'):\n",
    "    subprocess.check_call([sys.executable, '-m', 'git', 'clone', 'https://github.com/oleeveeuh/270FT.git'])\n",
    "else:\n",
    "    print('Repository already exists; fetching latest changes')\n",
    "    try:\n",
    "        subprocess.check_call(['git', '-C', '270FT', 'pull'])\n",
    "    except Exception as e:\n",
    "        print('git pull failed:', e)\n",
    "# Change working directory to repo root\n",
    "os.chdir('270FT')\n",
    "print('CWD:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Install required Python packages (may take a few minutes).\n",
    "# Install core packages first, then attempt bitsandbytes which may need a matching CUDA runtime.\n",
    "!pip install -q transformers datasets peft evaluate pyyaml huggingface_hub wandb\n",
    "# Try to install bitsandbytes; if it fails, you can retry with a wheel matching the runtime's CUDA\n",
    "try:\n",
    "    get_ipython().system('pip install -q bitsandbytes')\n",
    "    print('bitsandbytes installed')\n",
    "except Exception as e:\n",
    "    print('bitsandbytes install failed (you may still proceed if using a different runtime):', e)\n",
    "# Show GPU info if available\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d34f81",
   "metadata": {},
   "source": [
    "If `bitsandbytes` or other installs fail due to CUDA mismatches, try switching the Colab runtime GPU type (or use a notebook with a supported CUDA version). You can also skip installing `bitsandbytes` if you only want CPU runs, but the full QLoRA workflow requires a CUDA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171da73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) (Optional) Preprocess raw data into processed JSONL files.\n",
    "# This will create/update data/processed/train.jsonl, validation.jsonl, and test.jsonl\n",
    "!python preprocess/load_and_prepare.py --raw_dir data/raw --processed_dir data/processed --validation_split 0.15 --test_split 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) (Optional) Inspect processed files sizes and line counts\n",
    "!ls -lh data/processed || true\n",
    "!wc -l data/processed/*.jsonl || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e16ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Provide Hugging Face token (if required).\n",
    "# Use this cell to securely input a token if you need to access gated models.\n",
    "from getpass import getpass\n",
    "token = getpass('Hugging Face token (leave blank if not needed): ')\n",
    "import os\n",
    "if token:\n",
    "    os.environ['HF_TOKEN'] = token\n",
    "    # Also login via huggingface-cli for convenience\n",
    "    try:\n",
    "        get_ipython().system('huggingface-cli login --token \"$HF_TOKEN\"')\n",
    "    except Exception as e:\n",
    "        print('Automatic huggingface-cli login failed; token stored in HF_TOKEN')\n",
    "    print('HF_TOKEN set in environment')\n",
    "else:\n",
    "    print('No token provided; proceeding without HF token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b) Disable W&B interactive logging to avoid login prompts during automated runs\n",
    "import os\n",
    "os.environ['WANDB_MODE'] = 'offline'\n",
    "print('W&B offline mode set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eaf17e",
   "metadata": {},
   "source": "Now run the full training script below. The script `training/train_dual_lora.py` will look for `data/processed/train.jsonl`, `validation.jsonl`, and `test.jsonl` in `data/processed` and will use the models listed in `configs/training_config.yaml`.\n\n**Note about test data**: The test set is reserved for human-in-the-loop evaluation after training. During training, only the validation set is used for evaluation metrics. The training will log:\n- **Training loss** every 10 steps\n- **Validation loss** at the end of each epoch\n- Metrics to W&B (if enabled) or console output"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Run the training script (this will start QLoRA fine-tuning).\n",
    "# Note: training will log to W&B if enabled in the config; we set W&B to offline above to avoid interactive prompts.\n",
    "# Use unbuffered output so logs stream in Colab\n",
    "!python -u training/train_dual_lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2214c4f",
   "metadata": {},
   "source": "**Training Monitoring:**\nThe training script logs losses and metrics in multiple ways:\n1. **Console output**: Training loss logged every 10 steps, validation metrics at each epoch\n2. **W&B (if enabled)**: Full training/validation curves, model checkpoints\n3. **Checkpoints**: Saved every 500 steps to the output directory (keeping last 3)\n\n**Troubleshooting tips:**\n- If you see `FileNotFoundError` complaining about missing `test.jsonl`, re-run the preprocessing cell or ensure `data/processed/test.jsonl` exists.\n- If you get `bitsandbytes` import errors, try installing a different `bitsandbytes` wheel that matches the Colab CUDA runtime or switch runtime.\n- If training runs out of VRAM, reduce `batch_size` in `configs/training_config.yaml` or switch to a larger GPU.\n- If you prefer to persist checkpoints to Google Drive, create a folder in Drive and update `configs/training_config.yaml` output paths to point inside `/content/drive/MyDrive/...`.\n\n**Next steps after training:**\n- Run inference on test questions using the trained adapter\n- Use human-in-the-loop review to evaluate the quality of generated solutions\n- The test set at `data/processed/test.jsonl` contains questions without solutions (intentional for unbiased evaluation)"
  },
  {
   "cell_type": "markdown",
   "id": "bavasgbb4hv",
   "source": "## Understanding Your Results\n\n### Automated Metrics (for questions with reference solutions)\n- **Exact Match Rate**: Percentage of solutions that exactly match your reference (strict comparison)\n- **BLEU Score**: Token-level similarity score (0-1 scale)\n  - 0.0-0.1: Poor match\n  - 0.1-0.3: Fair match  \n  - 0.3-0.5: Good match\n  - 0.5+: Excellent match\n\n### Quality Checks (for all questions)\nAll generated solutions are automatically checked for:\n- ✓/✗ Has algorithm/pseudocode section\n- ✓/✗ Has runtime analysis (Big-O notation)\n- ✓/✗ Has proof keywords (proof, correctness, invariant, etc.)\n- ✓/✗ Has code structure (for/while/if statements)\n- Detected complexity notations\n- Length validation\n\n### Human Review Workflow\nFor questions without reference solutions:\n1. Download the `human_review_*.csv` file (cell above)\n2. Open in Excel or Google Sheets\n3. Review each generated solution\n4. Fill in the \"Rating (1-5)\" column:\n   - 1 = Incorrect/useless\n   - 2 = Major issues\n   - 3 = Acceptable but flawed\n   - 4 = Good with minor issues\n   - 5 = Excellent\n5. Add comments explaining your rating\n6. Calculate average rating and % of items rated 4-5\n\nFor complete documentation, see [EVALUATION_GUIDE.md](../EVALUATION_GUIDE.md) and [EVALUATION_QUICKSTART.md](../EVALUATION_QUICKSTART.md)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ji60lfihyyo",
   "source": "# 11) Download results for human review\n# This cell creates downloadable files:\n# - evaluation_*.json: Full automated metrics and quality checks\n# - human_review_*.csv: Template for manual review of items without solutions\n\nfrom google.colab import files\nfrom pathlib import Path\nimport os\n\nresults_dir = Path('results')\nif results_dir.exists():\n    # Download JSON results\n    json_files = list(results_dir.glob('evaluation_*.json'))\n    if json_files:\n        latest_json = max(json_files, key=os.path.getmtime)\n        print(f\"Downloading: {latest_json.name}\")\n        files.download(str(latest_json))\n    \n    # Download CSV for human review\n    csv_files = list(results_dir.glob('human_review_*.csv'))\n    if csv_files:\n        latest_csv = max(csv_files, key=os.path.getmtime)\n        print(f\"Downloading: {latest_csv.name}\")\n        files.download(str(latest_csv))\n        print(\"\\\\n✓ Open the CSV in Excel or Google Sheets to complete human review\")\n        print(\"  Fill in 'Rating (1-5)' and 'Comments' columns\")\n    else:\n        print(\"No CSV for human review (all items have reference solutions)\")\nelse:\n    print(\"Results directory not found. Run evaluation first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nsyfq3i9vq",
   "source": "# 10) View evaluation results summary\nimport json\nfrom pathlib import Path\nimport os\n\nresults_dir = Path('results')\nif results_dir.exists():\n    # Find the most recent evaluation JSON\n    json_files = list(results_dir.glob('evaluation_*.json'))\n    if json_files:\n        latest_json = max(json_files, key=os.path.getmtime)\n        \n        with open(latest_json, 'r') as f:\n            results = json.load(f)\n        \n        print(f\"{'='*60}\")\n        print(f\"EVALUATION RESULTS: {results['model_name']}\")\n        print(f\"{'='*60}\\\\n\")\n        \n        print(f\"Total Test Items: {results['total_items']}\")\n        print(f\"  Items with reference solutions: {results['items_with_solutions']}\")\n        print(f\"  Items needing human review: {results['items_without_solutions']}\\\\n\")\n        \n        if results['items_with_solutions'] > 0:\n            print(f\"AUTOMATED METRICS (on {results['items_with_solutions']} items with solutions):\")\n            print(f\"  Exact Match Rate: {results['automated_metrics']['exact_match_rate']:.4f} ({results['automated_metrics']['exact_matches']}/{results['items_with_solutions']} matched)\")\n            print(f\"  Average BLEU Score: {results['automated_metrics']['avg_bleu_score']:.4f}\")\n            print(f\"    (0.0-0.1: Poor, 0.1-0.3: Fair, 0.3-0.5: Good, 0.5+: Excellent)\\\\n\")\n        \n        if results['items_without_solutions'] > 0:\n            print(f\"HUMAN REVIEW NEEDED ({results['items_without_solutions']} items):\")\n            print(f\"  CSV template exported for manual review\")\n            print(f\"  Quality pre-checks completed\\\\n\")\n        \n        print(f\"\\\\nDetailed results saved to: {latest_json}\")\n    else:\n        print(\"No evaluation results found. Run evaluation first.\")\nelse:\n    print(\"Results directory not found. Run evaluation first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "o1jcqm4u6tj",
   "source": "# 9) Run hybrid evaluation (automated metrics + quality checks)\n# This will:\n# - Generate solutions for all test questions using your fine-tuned model\n# - Run automated metrics (BLEU, exact match) for questions with reference solutions\n# - Run quality checks (structure, completeness) for all questions\n# - Flag questions without solutions for human review\n# - Export results to JSON and CSV\n\n!python -u evaluation/evaluate_with_solutions.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xyhvce0ebni",
   "source": "# 8b) Run the reference solutions script\n# Only run this after editing REFERENCE_SOLUTIONS in the cell above\n!python temp_add_solutions.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1ginttgkiqo",
   "source": "# 8) Add reference solutions for test questions (where you have them)\n# Edit the REFERENCE_SOLUTIONS dictionary below to add your solutions\n\nreference_solutions_script = \"\"\"\nimport json\nfrom pathlib import Path\n\n# ADD YOUR REFERENCE SOLUTIONS HERE\n# Format: question ID (0-indexed) -> reference solution\n# Leave empty string \"\" for questions without solutions (will need human review)\nREFERENCE_SOLUTIONS = {\n    0: \\\"\\\"\\\"\n    # Your reference solution for test question 0 goes here\n    # Include algorithm, runtime analysis, and proof\n    # Or leave as empty string if you don't have a solution\n    \\\"\\\"\\\",\n\n    # Add more as needed...\n    # 1: \"\",  # No solution - will be flagged for human review\n    # 2: \"\\\"\\\"\\\"Your reference solution for question 2\\\"\\\"\\\",\n}\n\n# Load existing test data\nproject_root = Path.cwd()\ntest_input_path = project_root / \"data\" / \"processed\" / \"test.jsonl\"\ntest_output_path = project_root / \"data\" / \"processed\" / \"test_with_solutions.jsonl\"\n\nif not test_input_path.exists():\n    print(f\"Error: {test_input_path} not found\")\n    exit(1)\n\n# Load questions\nquestions = []\nwith open(test_input_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        line = line.strip()\n        if line:\n            questions.append(json.loads(line))\n\nprint(f\"Loaded {len(questions)} test questions\")\n\n# Add solutions where available\nitems_with_solutions = 0\nitems_without_solutions = 0\n\noutput_items = []\nfor idx, item in enumerate(questions):\n    solution = REFERENCE_SOLUTIONS.get(idx, \"\").strip()\n\n    if solution:\n        item[\"solution\"] = solution\n        items_with_solutions += 1\n    else:\n        # Don't add solution field - will trigger human review\n        items_without_solutions += 1\n\n    output_items.append(item)\n\n# Save updated test data\nwith open(test_output_path, 'w', encoding='utf-8') as f:\n    for item in output_items:\n        f.write(json.dumps(item, ensure_ascii=False) + '\\\\n')\n\nprint(f\"\\\\nSaved to: {test_output_path}\")\nprint(f\"  {items_with_solutions} items with reference solutions\")\nprint(f\"  {items_without_solutions} items without solutions (will need human review)\")\n\"\"\"\n\n# Write the script temporarily\nwith open('temp_add_solutions.py', 'w') as f:\n    f.write(reference_solutions_script)\n\nprint(\"✓ Script created. Edit REFERENCE_SOLUTIONS in the cell above, then run:\")\nprint(\"  !python temp_add_solutions.py\")\nprint(\"\\\\nOr skip this cell if you don't have reference solutions (all items will need human review)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uewm7emxvde",
   "source": "## Hybrid Evaluation: Automated Metrics + Human Review\n\nAfter training, evaluate your model using a hybrid approach:\n- **Automated metrics** (BLEU, exact match) for questions where you have reference solutions\n- **Quality checks** for all generated solutions\n- **Human review** for questions without reference solutions\n\nThis section will:\n1. Add your reference solutions to the test data\n2. Run evaluation (automated + quality checks)\n3. Generate results in JSON and CSV formats for review",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}