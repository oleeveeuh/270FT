{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e91fd273",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e91fd273",
        "outputId": "0b4ef9de-274b-422c-d809-ec5db96ebc3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1) (Optional) Mount Google Drive to persist checkpoints\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print('Not running in Colab or drive mount failed:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cadcdeaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cadcdeaf",
        "outputId": "f9fc121d-8208-453e-930c-f62a2d4fa179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changing directory from /content/270FT to /content\n",
            "Repository already exists at /content/270FT; fetching latest changes\n",
            "Changing directory from /content to /content/270FT\n",
            "CWD: /content/270FT\n"
          ]
        }
      ],
      "source": [
        "# 2) Clone the repository (or pull if already present)\n",
        "import os, subprocess, sys\n",
        "\n",
        "repo_name = '270FT'\n",
        "# The notebook typically starts in /content in Colab.\n",
        "# Define the absolute path to the intended repository location.\n",
        "intended_repo_parent_path = '/content'\n",
        "repo_absolute_path = os.path.join(intended_repo_parent_path, repo_name)\n",
        "\n",
        "# Ensure we are in the intended parent directory before any git operations\n",
        "# This prevents cloning into nested directories if CWD was changed previously\n",
        "if os.getcwd() != intended_repo_parent_path:\n",
        "    print(f\"Changing directory from {os.getcwd()} to {intended_repo_parent_path}\")\n",
        "    os.chdir(intended_repo_parent_path)\n",
        "\n",
        "if not os.path.exists(repo_absolute_path):\n",
        "    print(f'Cloning repository into {repo_absolute_path}')\n",
        "    # Clone directly into the target directory (will create repo_name inside CWD, which is now /content)\n",
        "    subprocess.check_call(['git', 'clone', 'https://github.com/oleeveeuh/270FT.git', repo_name])\n",
        "else:\n",
        "    print(f'Repository already exists at {repo_absolute_path}; fetching latest changes')\n",
        "    try:\n",
        "        # Use -C with the absolute path to ensure git pull operates on the correct directory\n",
        "        subprocess.check_call(['git', '-C', repo_absolute_path, 'pull'])\n",
        "    except Exception as e:\n",
        "        print('git pull failed:', e)\n",
        "\n",
        "# Ensure the working directory is always the absolute path of the repository root\n",
        "# This ensures that subsequent runs of the cell always land in the correct CWD\n",
        "# and prevents issues with relative chdir calls.\n",
        "if os.getcwd() != repo_absolute_path:\n",
        "    print(f\"Changing directory from {os.getcwd()} to {repo_absolute_path}\")\n",
        "    os.chdir(repo_absolute_path)\n",
        "\n",
        "print('CWD:', os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d19d8b99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d19d8b99",
        "outputId": "a0003458-563c-4177-eb7a-deca31e0d655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hbitsandbytes installed\n",
            "Fri Nov 28 05:31:21 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# 3) Install required Python packages (may take a few minutes).\n",
        "# Install core packages first, then attempt bitsandbytes which may need a matching CUDA runtime.\n",
        "!pip install -q transformers datasets peft evaluate pyyaml huggingface_hub wandb\n",
        "# Try to install bitsandbytes; if it fails, you can retry with a wheel matching the runtime's CUDA\n",
        "try:\n",
        "    get_ipython().system('pip install -q bitsandbytes')\n",
        "    print('bitsandbytes installed')\n",
        "except Exception as e:\n",
        "    print('bitsandbytes install failed (you may still proceed if using a different runtime):', e)\n",
        "# Show GPU info if available\n",
        "!nvidia-smi || true"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d34f81",
      "metadata": {
        "id": "63d34f81"
      },
      "source": [
        "If `bitsandbytes` or other installs fail due to CUDA mismatches, try switching the Colab runtime GPU type (or use a notebook with a supported CUDA version). You can also skip installing `bitsandbytes` if you only want CPU runs, but the full QLoRA workflow requires a CUDA GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "171da73a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "171da73a",
        "outputId": "e59394cd-b494-428c-a6f7-93ce646d612e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: pdfplumber not installed. PDF processing will be disabled.\n",
            "Install with: pip install pdfplumber\n",
            "Loading tokenizer: gpt2\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 186kB/s]\n",
            "config.json: 100% 665/665 [00:00<00:00, 6.06MB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 20.5MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 65.3MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 24.7MB/s]\n",
            "Scanning for files in: /content/270FT/data/raw\n",
            "No subdirectories detected. Using automatic splitting based on filename patterns...\n",
            "Found 0 training files, 0 validation files, and 0 test files\n",
            "\n",
            "Processing training files...\n",
            "\n",
            "Processing validation files...\n",
            "\n",
            "Processing test files...\n",
            "\n",
            "Saving processed data to: /content/270FT/data/processed\n",
            "\n",
            "[OK] Processed 0 training examples\n",
            "[OK] Processed 0 test examples\n",
            "[OK] Saved to /content/270FT/data/processed/train.jsonl\n",
            "[OK] Saved to /content/270FT/data/processed/test.jsonl\n"
          ]
        }
      ],
      "source": [
        "# 4) (Optional) Preprocess raw data into processed JSONL files.\n",
        "# This will create/update data/processed/train.jsonl, validation.jsonl, and test.jsonl\n",
        "!python preprocess/load_and_prepare.py --raw_dir data/raw --processed_dir data/processed --validation_split 0.15 --test_split 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7f7d6b8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f7d6b8e",
        "outputId": "23d1bccc-b93c-4f45-857d-6bb3d847662d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 264K\n",
            "-rw-r--r-- 1 root root  11K Nov 28 05:32 test.jsonl\n",
            "-rw-r--r-- 1 root root 207K Nov 28 05:32 train.jsonl\n",
            "-rw-r--r-- 1 root root  43K Nov 28 05:32 validation.jsonl\n",
            "     9 data/processed/test.jsonl\n",
            "   111 data/processed/train.jsonl\n",
            "    16 data/processed/validation.jsonl\n",
            "   136 total\n"
          ]
        }
      ],
      "source": [
        "# 5) (Optional) Inspect processed files sizes and line counts\n",
        "!ls -lh data/processed || true\n",
        "!wc -l data/processed/*.jsonl || true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13e16ac8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13e16ac8",
        "outputId": "3f21bb0c-aee1-44b0-b763-544e65adcf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging Face token (leave blank if not needed): 路路路路路路路路路路\n",
            "\u001b[33m锔  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `270ft` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "HF_TOKEN set in environment\n"
          ]
        }
      ],
      "source": [
        "# 6) Provide Hugging Face token (if required).\n",
        "# Use this cell to securely input a token if you need to access gated models.\n",
        "from getpass import getpass\n",
        "token = getpass('Hugging Face token (leave blank if not needed): ')\n",
        "import os\n",
        "if token:\n",
        "    os.environ['HF_TOKEN'] = token\n",
        "    # Also login via huggingface-cli for convenience\n",
        "    try:\n",
        "        get_ipython().system('huggingface-cli login --token \"$HF_TOKEN\"')\n",
        "    except Exception as e:\n",
        "        print('Automatic huggingface-cli login failed; token stored in HF_TOKEN')\n",
        "    print('HF_TOKEN set in environment')\n",
        "else:\n",
        "    print('No token provided; proceeding without HF token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5cf3fac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf3fac9",
        "outputId": "1581e27e-c095-402b-b86e-e0638d97b7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B offline mode set\n"
          ]
        }
      ],
      "source": [
        "# 6b) Disable W&B interactive logging to avoid login prompts during automated runs\n",
        "import os\n",
        "os.environ['WANDB_MODE'] = 'offline'\n",
        "print('W&B offline mode set')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0eaf17e",
      "metadata": {
        "id": "c0eaf17e"
      },
      "source": [
        "Now run the full training script below. The script `training/train_dual_lora.py` will look for `data/processed/train.jsonl`, `validation.jsonl`, and `test.jsonl` in `data/processed` and will use the models listed in `configs/training_config.yaml`.\n",
        "\n",
        "**Note about test data**: The test set is reserved for human-in-the-loop evaluation after training. During training, only the validation set is used for evaluation metrics. The training will log:\n",
        "- **Training loss** every 10 steps\n",
        "- **Validation loss** at the end of each epoch\n",
        "- Metrics to W&B (if enabled) or console output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1b02eac6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b02eac6",
        "outputId": "97e9ae44-8eac-4afd-deeb-566ba45b7aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 06:31:51.153634: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 06:31:51.172690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764311511.194849   20251 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764311511.201477   20251 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764311511.218852   20251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764311511.218895   20251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764311511.218898   20251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764311511.218901   20251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 06:31:51.224121: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Found validation data: /content/270FT/data/processed/validation.jsonl\n",
            "Loading training dataset...\n",
            "\n",
            "No --models argument specified. Will train all 8 models in config.\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 2652.23 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "AUTHENTICATION WARNING\n",
            "============================================================\n",
            "\n",
            "The model 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' requires Hugging Face authentication.\n",
            "\n",
            "To authenticate, run one of the following:\n",
            "  1. Run: huggingface-cli login\n",
            "  2. Or: python -c 'from huggingface_hub import login; login()'\n",
            "  3. Or set environment variable: export HF_TOKEN=your_token\n",
            "\n",
            "For LLaMA models, you may also need to:\n",
            "  - Request access at: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
            "  - Accept the model's terms of use\n",
            "\n",
            "Attempting to load model anyway...\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "Gradient checkpointing enabled (memory optimization)\n",
            "trainable params: 3,153,920 || all params: 1,103,202,304 || trainable%: 0.2859\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 1313.88 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moliau\u001b[0m (\u001b[33moliau-usc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m猗\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m猓\u001b[0m setting up run bgvogiy8 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m猓\u001b[0m setting up run bgvogiy8 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m猓\u001b[0m setting up run bgvogiy8 (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/270FT/wandb/run-20251128_063216-bgvogiy8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTinyLlama-1.1B-Chat-v1.0_qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/oliau-usc/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/oliau-usc/huggingface/runs/bgvogiy8\u001b[0m\n",
            "  0% 0/56 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "{'loss': 3.0639, 'grad_norm': 1.9898761510849, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 2.8796, 'grad_norm': 2.2414135932922363, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 2.8746, 'grad_norm': 1.7682722806930542, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 2.8895, 'grad_norm': 1.6334363222122192, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 2.7685, 'grad_norm': 1.5890802145004272, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [00:48<00:00,  1.37it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:00, 16.60it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01, 10.52it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  9.41it/s]\u001b[A\n",
            " 50% 8/16 [00:00<00:00,  8.94it/s]\u001b[A\n",
            " 56% 9/16 [00:00<00:00,  8.80it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:00,  8.68it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  8.57it/s]\u001b[A\n",
            " 75% 12/16 [00:01<00:00,  8.50it/s]\u001b[A\n",
            " 81% 13/16 [00:01<00:00,  8.44it/s]\u001b[A\n",
            " 88% 14/16 [00:01<00:00,  8.38it/s]\u001b[A\n",
            " 94% 15/16 [00:01<00:00,  8.31it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_exact_match': 0.0, 'eval_bleu': 0.26020960538725896, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 2.0680203437805176, 'eval_runtime': 4.4196, 'eval_samples_per_second': 3.62, 'eval_steps_per_second': 3.62, 'epoch': 1.0}\n",
            "100% 56/56 [00:52<00:00,  1.37it/s]\n",
            "100% 16/16 [00:04<00:00,  8.32it/s]\u001b[A\n",
            "{'train_runtime': 55.6071, 'train_samples_per_second': 1.996, 'train_steps_per_second': 1.007, 'train_loss': 2.8602047307150706, 'epoch': 1.0}\n",
            "100% 56/56 [00:53<00:00,  1.05it/s]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:03<00:00,  4.20it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/tinyllama_1.1b_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: HuggingFaceTB/SmolLM3-3B\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for HuggingFaceTB/SmolLM3-3B...\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 2938.67 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: HuggingFaceTB/SmolLM3-3B\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "Loading checkpoint shards: 100% 2/2 [00:30<00:00, 15.43s/it]\n",
            "Gradient checkpointing enabled (memory optimization)\n",
            "trainable params: 7,557,120 || all params: 3,082,655,744 || trainable%: 0.2451\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 1121.68 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "{'loss': 3.1374, 'grad_norm': 1.739869475364685, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 2.8958, 'grad_norm': 1.4988129138946533, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 2.9422, 'grad_norm': 2.0134737491607666, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 2.7963, 'grad_norm': 2.2332682609558105, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 2.8089, 'grad_norm': 1.7332450151443481, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [01:17<00:00,  1.20s/it]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:01, 10.47it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01,  6.59it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:01,  6.12it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  5.84it/s]\u001b[A\n",
            " 44% 7/16 [00:01<00:01,  5.65it/s]\u001b[A\n",
            " 50% 8/16 [00:01<00:01,  5.53it/s]\u001b[A\n",
            " 56% 9/16 [00:01<00:01,  5.45it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:01,  5.38it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  5.34it/s]\u001b[A\n",
            " 75% 12/16 [00:02<00:00,  5.31it/s]\u001b[A\n",
            " 81% 13/16 [00:02<00:00,  5.29it/s]\u001b[A\n",
            " 88% 14/16 [00:02<00:00,  5.28it/s]\u001b[A\n",
            " 94% 15/16 [00:02<00:00,  5.26it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_exact_match': 0.0, 'eval_bleu': 0.33520387853132333, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 1.8637111186981201, 'eval_runtime': 6.308, 'eval_samples_per_second': 2.536, 'eval_steps_per_second': 2.536, 'epoch': 1.0}\n",
            "100% 56/56 [01:24<00:00,  1.20s/it]\n",
            "100% 16/16 [00:06<00:00,  5.25it/s]\u001b[A\n",
            "{'train_runtime': 85.4938, 'train_samples_per_second': 1.298, 'train_steps_per_second': 0.655, 'train_loss': 2.8698616538728987, 'epoch': 1.0}\n",
            "100% 56/56 [01:25<00:00,  1.53s/it]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:06<00:00,  2.66it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/smollm3_3b_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: ministral/Ministral-3b-instruct\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for ministral/Ministral-3b-instruct...\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 3009.02 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: ministral/Ministral-3b-instruct\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "Loading checkpoint shards: 100% 3/3 [00:33<00:00, 11.29s/it]\n",
            "Gradient checkpointing enabled (memory optimization)\n",
            "trainable params: 4,587,520 || all params: 3,320,303,616 || trainable%: 0.1382\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 1147.12 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "{'loss': 4.1001, 'grad_norm': 3.4180634021759033, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 3.8514, 'grad_norm': 3.6670031547546387, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 3.7698, 'grad_norm': 3.883187770843506, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 3.6759, 'grad_norm': 3.4298741817474365, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 3.6175, 'grad_norm': 3.218595504760742, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [00:31<00:00,  2.07it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 19% 3/16 [00:00<00:00, 19.74it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:00, 15.73it/s]\u001b[A\n",
            " 44% 7/16 [00:00<00:00, 14.41it/s]\u001b[A\n",
            " 56% 9/16 [00:00<00:00, 13.83it/s]\u001b[A\n",
            " 69% 11/16 [00:00<00:00, 13.52it/s]\u001b[A\n",
            " 81% 13/16 [00:00<00:00, 13.31it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_exact_match': 0.0, 'eval_bleu': 0.1727824364546176, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 2.9879751205444336, 'eval_runtime': 3.2166, 'eval_samples_per_second': 4.974, 'eval_steps_per_second': 4.974, 'epoch': 1.0}\n",
            "100% 56/56 [00:34<00:00,  2.07it/s]\n",
            "100% 16/16 [00:03<00:00, 13.20it/s]\u001b[A\n",
            "{'train_runtime': 35.1964, 'train_samples_per_second': 3.154, 'train_steps_per_second': 1.591, 'train_loss': 3.7533624172210693, 'epoch': 1.0}\n",
            "100% 56/56 [00:35<00:00,  1.59it/s]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:03<00:00,  5.17it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/ministral_3b_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: deepseek-ai/deepseek-llm-7b-chat\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for deepseek-ai/deepseek-llm-7b-chat...\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 1785.15 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: deepseek-ai/deepseek-llm-7b-chat\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "Loading checkpoint shards: 100% 2/2 [01:11<00:00, 35.63s/it]\n",
            "Gradient checkpointing enabled (memory optimization)\n",
            "trainable params: 9,369,600 || all params: 6,919,735,296 || trainable%: 0.1354\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 779.99 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "{'loss': 3.0525, 'grad_norm': 1.8483484983444214, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 2.7044, 'grad_norm': 0.8287659883499146, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 2.7325, 'grad_norm': 1.0036288499832153, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 2.6791, 'grad_norm': 1.0617082118988037, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 2.6318, 'grad_norm': 0.8787851333618164, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [01:05<00:00,  1.01s/it]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:01, 12.02it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01,  7.73it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:01,  7.19it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  6.82it/s]\u001b[A\n",
            " 44% 7/16 [00:00<00:01,  6.59it/s]\u001b[A\n",
            " 50% 8/16 [00:01<00:01,  6.42it/s]\u001b[A\n",
            " 56% 9/16 [00:01<00:01,  6.29it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:00,  6.22it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  6.19it/s]\u001b[A\n",
            " 75% 12/16 [00:01<00:00,  6.17it/s]\u001b[A\n",
            " 81% 13/16 [00:01<00:00,  6.14it/s]\u001b[A\n",
            " 88% 14/16 [00:02<00:00,  6.02it/s]\u001b[A\n",
            " 94% 15/16 [00:02<00:00,  6.01it/s]\u001b[A\n",
            "100% 16/16 [00:02<00:00,  6.04it/s]\u001b[A\n",
            "{'eval_exact_match': 0.0, 'eval_bleu': 0.3251673710749406, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 1.816412329673767, 'eval_runtime': 6.0053, 'eval_samples_per_second': 2.664, 'eval_steps_per_second': 2.664, 'epoch': 1.0}\n",
            "\n",
            "100% 56/56 [01:11<00:00,  1.01s/it]\n",
            "{'train_runtime': 72.9804, 'train_samples_per_second': 1.521, 'train_steps_per_second': 0.767, 'train_loss': 2.718794754573277, 'epoch': 1.0}\n",
            "100% 56/56 [01:12<00:00,  1.30s/it]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:05<00:00,  3.03it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/deepseek_7b_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: Qwen/Qwen2.5-7B-Instruct\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for Qwen/Qwen2.5-7B-Instruct...\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 2288.75 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: Qwen/Qwen2.5-7B-Instruct\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "Loading checkpoint shards: 100% 4/4 [01:18<00:00, 19.73s/it]\n",
            "Gradient checkpointing enabled (memory optimization)\n",
            "trainable params: 10,092,544 || all params: 7,625,709,056 || trainable%: 0.1323\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 1007.11 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "{'loss': 2.7046, 'grad_norm': 1.8812257051467896, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 2.3667, 'grad_norm': 1.4368503093719482, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 2.4344, 'grad_norm': 1.4475914239883423, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 2.311, 'grad_norm': 1.172516107559204, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 2.3455, 'grad_norm': 1.331578254699707, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [01:02<00:00,  1.05it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:01, 12.63it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01,  7.90it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:01,  7.35it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  6.97it/s]\u001b[A\n",
            " 44% 7/16 [00:00<00:01,  6.74it/s]\u001b[A\n",
            " 50% 8/16 [00:01<00:01,  6.55it/s]\u001b[A\n",
            " 56% 9/16 [00:01<00:01,  6.41it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:00,  6.32it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  6.23it/s]\u001b[A\n",
            " 75% 12/16 [00:01<00:00,  6.18it/s]\u001b[A\n",
            " 81% 13/16 [00:01<00:00,  6.16it/s]\u001b[A\n",
            " 88% 14/16 [00:02<00:00,  6.03it/s]\u001b[A\n",
            " 94% 15/16 [00:02<00:00,  6.02it/s]\u001b[A\n",
            "100% 16/16 [00:02<00:00,  6.06it/s]\u001b[A\n",
            "{'eval_exact_match': 0.0, 'eval_bleu': 0.3498133513288292, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 1.6115384101867676, 'eval_runtime': 6.384, 'eval_samples_per_second': 2.506, 'eval_steps_per_second': 2.506, 'epoch': 1.0}\n",
            "\n",
            "100% 56/56 [01:09<00:00,  1.05it/s]\n",
            "{'train_runtime': 70.3258, 'train_samples_per_second': 1.578, 'train_steps_per_second': 0.796, 'train_loss': 2.3952914646693637, 'epoch': 1.0}\n",
            "100% 56/56 [01:10<00:00,  1.26s/it]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:05<00:00,  2.78it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/qwen2.5_7b_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: microsoft/phi-3.5-mini-instruct\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for microsoft/phi-3.5-mini-instruct...\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 2526.88 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: microsoft/phi-3.5-mini-instruct\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "  Using eager attention for Phi model (compatibility fix)\n",
            "  Disabled gradient checkpointing for Phi model (cache compatibility)\n",
            "[OK] Applied DynamicCache compatibility patch for Phi-3.5\n",
            "WARNING:transformers_modules.microsoft.phi_hyphen_3_dot_5_hyphen_mini_hyphen_instruct.3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.phi_hyphen_3_dot_5_hyphen_mini_hyphen_instruct.3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Loading checkpoint shards: 100% 2/2 [00:40<00:00, 20.32s/it]\n",
            "Gradient checkpointing disabled for Phi model (cache compatibility)\n",
            "trainable params: 2,228,224 || all params: 3,823,307,776 || trainable%: 0.0583\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 1172.60 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Disabled caching for Phi model evaluation (prevents attention shape mismatches)\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "  0% 0/56 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "WARNING:transformers_modules.microsoft.phi_hyphen_3_dot_5_hyphen_mini_hyphen_instruct.3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 3.3448, 'grad_norm': 1.3690729141235352, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 2.793, 'grad_norm': 0.9674808979034424, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 2.9437, 'grad_norm': 1.071316123008728, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 2.8448, 'grad_norm': 0.7250054478645325, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 2.6758, 'grad_norm': 0.6905462145805359, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [00:44<00:00,  1.47it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:00, 17.34it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01, 10.97it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  9.84it/s]\u001b[A\n",
            " 50% 8/16 [00:00<00:00,  9.39it/s]\u001b[A\n",
            " 56% 9/16 [00:00<00:00,  9.23it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:00,  9.11it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  9.02it/s]\u001b[A\n",
            " 75% 12/16 [00:01<00:00,  8.96it/s]\u001b[A\n",
            " 81% 13/16 [00:01<00:00,  8.91it/s]\u001b[A\n",
            " 88% 14/16 [00:01<00:00,  8.87it/s]\u001b[A\n",
            " 94% 15/16 [00:01<00:00,  8.85it/s]\u001b[A\n",
            "100% 16/16 [00:01<00:00,  8.83it/s]\u001b[A\n",
            "{'eval_exact_match': 0.0, 'eval_bleu': 0.3108119475312861, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 1.7433159351348877, 'eval_runtime': 4.3196, 'eval_samples_per_second': 3.704, 'eval_steps_per_second': 3.704, 'epoch': 1.0}\n",
            "\n",
            "100% 56/56 [00:48<00:00,  1.47it/s]\n",
            "{'train_runtime': 49.6329, 'train_samples_per_second': 2.236, 'train_steps_per_second': 1.128, 'train_loss': 2.8852504321507046, 'epoch': 1.0}\n",
            "100% 56/56 [00:49<00:00,  1.13it/s]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:03<00:00,  4.35it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/phi3.5_mini_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: mistralai/Mistral-7B-Instruct-v0.3\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.3...\n",
            "  Set pad_token to eos_token\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 3251.85 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: mistralai/Mistral-7B-Instruct-v0.3\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "Loading checkpoint shards: 100% 3/3 [01:16<00:00, 25.53s/it]\n",
            "trainable params: 10,485,760 || all params: 7,258,509,312 || trainable%: 0.1445\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 1194.87 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "  0% 0/56 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "{'loss': 2.8224, 'grad_norm': 4.477843284606934, 'learning_rate': 0.000175, 'epoch': 0.18}\n",
            "{'loss': 2.3555, 'grad_norm': 3.917630195617676, 'learning_rate': 0.0001392857142857143, 'epoch': 0.36}\n",
            "{'loss': 2.4164, 'grad_norm': 3.917602062225342, 'learning_rate': 0.00010357142857142859, 'epoch': 0.54}\n",
            "{'loss': 2.3842, 'grad_norm': 3.2333672046661377, 'learning_rate': 6.785714285714286e-05, 'epoch': 0.72}\n",
            "{'loss': 2.3326, 'grad_norm': 3.961400270462036, 'learning_rate': 3.2142857142857144e-05, 'epoch': 0.9}\n",
            "100% 56/56 [01:11<00:00,  1.11s/it]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:01, 11.31it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01,  7.12it/s]\u001b[A\n",
            " 31% 5/16 [00:00<00:01,  6.63it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  6.32it/s]\u001b[A\n",
            " 44% 7/16 [00:01<00:01,  6.09it/s]\u001b[A\n",
            " 50% 8/16 [00:01<00:01,  5.92it/s]\u001b[A\n",
            " 56% 9/16 [00:01<00:01,  5.86it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:01,  5.80it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  5.77it/s]\u001b[A\n",
            " 75% 12/16 [00:01<00:00,  5.70it/s]\u001b[A\n",
            " 81% 13/16 [00:02<00:00,  5.68it/s]\u001b[A\n",
            " 88% 14/16 [00:02<00:00,  5.65it/s]\u001b[A\n",
            " 94% 15/16 [00:02<00:00,  5.66it/s]\u001b[A\n",
            "100% 16/16 [00:02<00:00,  5.68it/s]\u001b[A\n",
            "{'eval_exact_match': 0.0, 'eval_bleu': 0.33400109725359495, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 1.6483516693115234, 'eval_runtime': 5.3411, 'eval_samples_per_second': 2.996, 'eval_steps_per_second': 2.996, 'epoch': 1.0}\n",
            "\n",
            "100% 56/56 [01:17<00:00,  1.11s/it]\n",
            "{'train_runtime': 78.1788, 'train_samples_per_second': 1.42, 'train_steps_per_second': 0.716, 'train_loss': 2.424416644232614, 'epoch': 1.0}\n",
            "100% 56/56 [01:18<00:00,  1.40s/it]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:04<00:00,  3.25it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/mistral_7b_lora...\n",
            "\n",
            "============================================================\n",
            "Preparing dataset for: tiiuae/falcon-7b-instruct\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer for tiiuae/falcon-7b-instruct...\n",
            "  Set pad_token to eos_token\n",
            "Tokenizing training dataset (max_length=256)...\n",
            "  Tokenizing 111 examples...\n",
            "Tokenizing dataset: 100% 111/111 [00:00<00:00, 2057.63 examples/s]\n",
            "  Tokenization complete. Dataset size: 111\n",
            "Training dataset ready: 111 examples\n",
            "\n",
            "============================================================\n",
            "Training model: tiiuae/falcon-7b-instruct\n",
            "============================================================\n",
            "\n",
            "W&B logging disabled\n",
            "Loading model and tokenizer...\n",
            "  CUDA available: 1 device(s)\n",
            "  Disabled gradient checkpointing for Falcon model (cache compatibility)\n",
            "WARNING:transformers_modules.tiiuae.falcon_hyphen_7b_hyphen_instruct.8782b5c5d8c9290412416618f36a133653e85285.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [01:16<00:00, 38.45s/it]\n",
            "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
            "Gradient checkpointing disabled for Falcon model (cache compatibility)\n",
            "trainable params: 8,159,232 || all params: 6,929,879,936 || trainable%: 0.1177\n",
            "Disabled use_cache for Falcon model (KV cache compatibility with LoRA)\n",
            "Loading validation data from /content/270FT/data/processed/validation.jsonl...\n",
            "  Tokenizing 16 examples...\n",
            "Tokenizing dataset: 100% 16/16 [00:00<00:00, 829.24 examples/s]\n",
            "  Tokenization complete. Dataset size: 16\n",
            "Validation dataset size: 16\n",
            "Starting training...\n",
            "A ConfigError was raised whilst setting the number of model parameters in Weights & Biases config.\n",
            "  0% 0/56 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "{'loss': 3.3571, 'grad_norm': 1.6257460117340088, 'learning_rate': 0.00016785714285714288, 'epoch': 0.18}\n",
            "{'loss': 2.9221, 'grad_norm': 2.0087380409240723, 'learning_rate': 0.00013214285714285715, 'epoch': 0.36}\n",
            "{'loss': 2.9385, 'grad_norm': 2.0009024143218994, 'learning_rate': 9.642857142857143e-05, 'epoch': 0.54}\n",
            "{'loss': 2.8808, 'grad_norm': 1.9170503616333008, 'learning_rate': 6.0714285714285715e-05, 'epoch': 0.72}\n",
            "{'loss': 2.7913, 'grad_norm': 1.662218689918518, 'learning_rate': 2.5e-05, 'epoch': 0.9}\n",
            "100% 56/56 [00:49<00:00,  1.33it/s]\n",
            "  0% 0/16 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 2/16 [00:00<00:00, 16.60it/s]\u001b[A\n",
            " 25% 4/16 [00:00<00:01, 10.56it/s]\u001b[A\n",
            " 38% 6/16 [00:00<00:01,  9.27it/s]\u001b[A\n",
            " 50% 8/16 [00:00<00:00,  8.83it/s]\u001b[A\n",
            " 56% 9/16 [00:00<00:00,  8.64it/s]\u001b[A\n",
            " 62% 10/16 [00:01<00:00,  8.53it/s]\u001b[A\n",
            " 69% 11/16 [00:01<00:00,  8.42it/s]\u001b[A\n",
            " 75% 12/16 [00:01<00:00,  8.32it/s]\u001b[A\n",
            " 81% 13/16 [00:01<00:00,  8.27it/s]\u001b[A\n",
            " 88% 14/16 [00:01<00:00,  8.18it/s]\u001b[A\n",
            " 94% 15/16 [00:01<00:00,  8.14it/s]\u001b[A\n",
            "100% 16/16 [00:01<00:00,  8.17it/s]\u001b[A\n",
            "{'eval_exact_match': 0.0, 'eval_bleu': 0.25604270615052094, 'eval_symbolic_equivalence': 0.0, 'eval_loss': 2.0879228115081787, 'eval_runtime': 4.6206, 'eval_samples_per_second': 3.463, 'eval_steps_per_second': 3.463, 'epoch': 1.0}\n",
            "\n",
            "100% 56/56 [00:54<00:00,  1.33it/s]\n",
            "{'train_runtime': 54.9591, 'train_samples_per_second': 2.02, 'train_steps_per_second': 1.019, 'train_loss': 2.944838660103934, 'epoch': 1.0}\n",
            "100% 56/56 [00:54<00:00,  1.02it/s]\n",
            "Evaluating on validation set...\n",
            "100% 16/16 [00:04<00:00,  3.83it/s]\n",
            "\n",
            "Skipping test set evaluation during training (test data reserved for human-in-the-loop evaluation)\n",
            "Saving adapter to /content/270FT/models/falcon_7b_lora...\n",
            "\n",
            "============================================================\n",
            "TRAINING SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "  Evaluation Loss: 2.0680\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.2602\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: HuggingFaceTB/SmolLM3-3B\n",
            "  Evaluation Loss: 1.8637\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.3352\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: ministral/Ministral-3b-instruct\n",
            "  Evaluation Loss: 2.9880\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.1728\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: deepseek-ai/deepseek-llm-7b-chat\n",
            "  Evaluation Loss: 1.8164\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.3252\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: Qwen/Qwen2.5-7B-Instruct\n",
            "  Evaluation Loss: 1.6115\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.3498\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: microsoft/phi-3.5-mini-instruct\n",
            "  Evaluation Loss: 1.7433\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.3108\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: mistralai/Mistral-7B-Instruct-v0.3\n",
            "  Evaluation Loss: 1.6484\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.3340\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "Model: tiiuae/falcon-7b-instruct\n",
            "  Evaluation Loss: 2.0879\n",
            "  exact_match: 0.0000\n",
            "  bleu: 0.2560\n",
            "  symbolic_equivalence: 0.0000\n",
            "\n",
            "============================================================\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m:  View run \u001b[33mTinyLlama-1.1B-Chat-v1.0_qlora\u001b[0m at: \u001b[34m\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251128_063216-bgvogiy8/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 7) Run the training script (this will start QLoRA fine-tuning).\n",
        "# Note: training will log to W&B if enabled in the config; we set W&B to offline above to avoid interactive prompts.\n",
        "# Use unbuffered output so logs stream in Colab\n",
        "!python -u training/train_dual_lora.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2214c4f",
      "metadata": {
        "id": "d2214c4f"
      },
      "source": [
        "**Training Monitoring:**\n",
        "The training script logs losses and metrics in multiple ways:\n",
        "1. **Console output**: Training loss logged every 10 steps, validation metrics at each epoch\n",
        "2. **W&B (if enabled)**: Full training/validation curves, model checkpoints\n",
        "3. **Checkpoints**: Saved every 500 steps to the output directory (keeping last 3)\n",
        "\n",
        "**Troubleshooting tips:**\n",
        "- If you see `FileNotFoundError` complaining about missing `test.jsonl`, re-run the preprocessing cell or ensure `data/processed/test.jsonl` exists.\n",
        "- If you get `bitsandbytes` import errors, try installing a different `bitsandbytes` wheel that matches the Colab CUDA runtime or switch runtime.\n",
        "- If training runs out of VRAM, reduce `batch_size` in `configs/training_config.yaml` or switch to a larger GPU.\n",
        "- If you prefer to persist checkpoints to Google Drive, create a folder in Drive and update `configs/training_config.yaml` output paths to point inside `/content/drive/MyDrive/...`.\n",
        "\n",
        "**Next steps after training:**\n",
        "- Run inference on test questions using the trained adapter\n",
        "- Use human-in-the-loop review to evaluate the quality of generated solutions\n",
        "- The test set at `data/processed/test.jsonl` contains questions without solutions (intentional for unbiased evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bavasgbb4hv",
      "source": [
        "## Understanding Your Results\n",
        "\n",
        "### Automated Metrics (for questions with reference solutions)\n",
        "- **Exact Match Rate**: Percentage of solutions that exactly match your reference (strict comparison)\n",
        "- **BLEU Score**: Token-level similarity score (0-1 scale)\n",
        "  - 0.0-0.1: Poor match\n",
        "  - 0.1-0.3: Fair match  \n",
        "  - 0.3-0.5: Good match\n",
        "  - 0.5+: Excellent match\n",
        "\n",
        "### Quality Checks (for all questions)\n",
        "All generated solutions are automatically checked for:\n",
        "- / Has algorithm/pseudocode section\n",
        "- / Has runtime analysis (Big-O notation)\n",
        "- / Has proof keywords (proof, correctness, invariant, etc.)\n",
        "- / Has code structure (for/while/if statements)\n",
        "- Detected complexity notations\n",
        "- Length validation\n",
        "\n",
        "### Human Review Workflow\n",
        "For questions without reference solutions:\n",
        "1. Download the `human_review_*.csv` file (cell above)\n",
        "2. Open in Excel or Google Sheets\n",
        "3. Review each generated solution\n",
        "4. Fill in the \"Rating (1-5)\" column:\n",
        "   - 1 = Incorrect/useless\n",
        "   - 2 = Major issues\n",
        "   - 3 = Acceptable but flawed\n",
        "   - 4 = Good with minor issues\n",
        "   - 5 = Excellent\n",
        "5. Add comments explaining your rating\n",
        "6. Calculate average rating and % of items rated 4-5\n",
        "\n",
        "For complete documentation, see [EVALUATION_GUIDE.md](../EVALUATION_GUIDE.md) and [EVALUATION_QUICKSTART.md](../EVALUATION_QUICKSTART.md)"
      ],
      "metadata": {
        "id": "bavasgbb4hv"
      }
    },
    {
      "cell_type": "code",
      "id": "ji60lfihyyo",
      "source": [
        "# 11) Download results for human review\n",
        "# This cell creates downloadable files:\n",
        "# - evaluation_*.json: Full automated metrics and quality checks\n",
        "# - human_review_*.csv: Template for manual review of items without solutions\n",
        "\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "results_dir = Path('results')\n",
        "if results_dir.exists():\n",
        "    # Download JSON results\n",
        "    json_files = list(results_dir.glob('evaluation_*.json'))\n",
        "    if json_files:\n",
        "        latest_json = max(json_files, key=os.path.getmtime)\n",
        "        print(f\"Downloading: {latest_json.name}\")\n",
        "        files.download(str(latest_json))\n",
        "\n",
        "    # Download CSV for human review\n",
        "    csv_files = list(results_dir.glob('human_review_*.csv'))\n",
        "    if csv_files:\n",
        "        latest_csv = max(csv_files, key=os.path.getmtime)\n",
        "        print(f\"Downloading: {latest_csv.name}\")\n",
        "        files.download(str(latest_csv))\n",
        "        print(\"\\\\n Open the CSV in Excel or Google Sheets to complete human review\")\n",
        "        print(\"  Fill in 'Rating (1-5)' and 'Comments' columns\")\n",
        "    else:\n",
        "        print(\"No CSV for human review (all items have reference solutions)\")\n",
        "else:\n",
        "    print(\"Results directory not found. Run evaluation first.\")"
      ],
      "metadata": {
        "id": "ji60lfihyyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "nsyfq3i9vq",
      "source": [
        "# 10) View evaluation results summary\n",
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "results_dir = Path('results')\n",
        "if results_dir.exists():\n",
        "    # Find the most recent evaluation JSON\n",
        "    json_files = list(results_dir.glob('evaluation_*.json'))\n",
        "    if json_files:\n",
        "        latest_json = max(json_files, key=os.path.getmtime)\n",
        "\n",
        "        with open(latest_json, 'r') as f:\n",
        "            results = json.load(f)\n",
        "\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"EVALUATION RESULTS: {results['model_name']}\")\n",
        "        print(f\"{'='*60}\\\\n\")\n",
        "\n",
        "        print(f\"Total Test Items: {results['total_items']}\")\n",
        "        print(f\"  Items with reference solutions: {results['items_with_solutions']}\")\n",
        "        print(f\"  Items needing human review: {results['items_without_solutions']}\\\\n\")\n",
        "\n",
        "        if results['items_with_solutions'] > 0:\n",
        "            print(f\"AUTOMATED METRICS (on {results['items_with_solutions']} items with solutions):\")\n",
        "            print(f\"  Exact Match Rate: {results['automated_metrics']['exact_match_rate']:.4f} ({results['automated_metrics']['exact_matches']}/{results['items_with_solutions']} matched)\")\n",
        "            print(f\"  Average BLEU Score: {results['automated_metrics']['avg_bleu_score']:.4f}\")\n",
        "            print(f\"    (0.0-0.1: Poor, 0.1-0.3: Fair, 0.3-0.5: Good, 0.5+: Excellent)\\\\n\")\n",
        "\n",
        "        if results['items_without_solutions'] > 0:\n",
        "            print(f\"HUMAN REVIEW NEEDED ({results['items_without_solutions']} items):\")\n",
        "            print(f\"  CSV template exported for manual review\")\n",
        "            print(f\"  Quality pre-checks completed\\\\n\")\n",
        "\n",
        "        print(f\"\\\\nDetailed results saved to: {latest_json}\")\n",
        "    else:\n",
        "        print(\"No evaluation results found. Run evaluation first.\")\n",
        "else:\n",
        "    print(\"Results directory not found. Run evaluation first.\")"
      ],
      "metadata": {
        "id": "nsyfq3i9vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "o1jcqm4u6tj",
      "source": [
        "# 9) Run hybrid evaluation (automated metrics + quality checks)\n",
        "# This will:\n",
        "# - Generate solutions for all test questions using your fine-tuned model\n",
        "# - Run automated metrics (BLEU, exact match) for questions with reference solutions\n",
        "# - Run quality checks (structure, completeness) for all questions\n",
        "# - Flag questions without solutions for human review\n",
        "# - Export results to JSON and CSV\n",
        "\n",
        "!python -u evaluation/evaluate_with_solutions.py"
      ],
      "metadata": {
        "id": "o1jcqm4u6tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "xyhvce0ebni",
      "source": [
        "# 8b) Run the reference solutions script\n",
        "# Only run this after editing REFERENCE_SOLUTIONS in the cell above\n",
        "!python temp_add_solutions.py"
      ],
      "metadata": {
        "id": "xyhvce0ebni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "1ginttgkiqo",
      "source": [
        "# 8) Add reference solutions for test questions (where you have them)\n",
        "# Edit the REFERENCE_SOLUTIONS dictionary below to add your solutions\n",
        "\n",
        "reference_solutions_script = \"\"\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# ADD YOUR REFERENCE SOLUTIONS HERE\n",
        "# Format: question ID (0-indexed) -> reference solution\n",
        "# Leave empty string \"\" for questions without solutions (will need human review)\n",
        "REFERENCE_SOLUTIONS = {\n",
        "    0: \\\"\\\"\\\"\n",
        "    # Your reference solution for test question 0 goes here\n",
        "    # Include algorithm, runtime analysis, and proof\n",
        "    # Or leave as empty string if you don't have a solution\n",
        "    \\\"\\\"\\\",\n",
        "\n",
        "    # Add more as needed...\n",
        "    # 1: \"\",  # No solution - will be flagged for human review\n",
        "    # 2: \"\\\"\\\"\\\"Your reference solution for question 2\\\"\\\"\\\",\n",
        "}\n",
        "\n",
        "# Load existing test data\n",
        "project_root = Path.cwd()\n",
        "test_input_path = project_root / \"data\" / \"processed\" / \"test.jsonl\"\n",
        "test_output_path = project_root / \"data\" / \"processed\" / \"test_with_solutions.jsonl\"\n",
        "\n",
        "if not test_input_path.exists():\n",
        "    print(f\"Error: {test_input_path} not found\")\n",
        "    exit(1)\n",
        "\n",
        "# Load questions\n",
        "questions = []\n",
        "with open(test_input_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            questions.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(questions)} test questions\")\n",
        "\n",
        "# Add solutions where available\n",
        "items_with_solutions = 0\n",
        "items_without_solutions = 0\n",
        "\n",
        "output_items = []\n",
        "for idx, item in enumerate(questions):\n",
        "    solution = REFERENCE_SOLUTIONS.get(idx, \"\").strip()\n",
        "\n",
        "    if solution:\n",
        "        item[\"solution\"] = solution\n",
        "        items_with_solutions += 1\n",
        "    else:\n",
        "        # Don't add solution field - will trigger human review\n",
        "        items_without_solutions += 1\n",
        "\n",
        "    output_items.append(item)\n",
        "\n",
        "# Save updated test data\n",
        "with open(test_output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in output_items:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\\\n')\n",
        "\n",
        "print(f\"\\\\nSaved to: {test_output_path}\")\n",
        "print(f\"  {items_with_solutions} items with reference solutions\")\n",
        "print(f\"  {items_without_solutions} items without solutions (will need human review)\")\n",
        "\"\"\"\n",
        "\n",
        "# Write the script temporarily\n",
        "with open('temp_add_solutions.py', 'w') as f:\n",
        "    f.write(reference_solutions_script)\n",
        "\n",
        "print(\" Script created. Edit REFERENCE_SOLUTIONS in the cell above, then run:\")\n",
        "print(\"  !python temp_add_solutions.py\")\n",
        "print(\"\\\\nOr skip this cell if you don't have reference solutions (all items will need human review)\")"
      ],
      "metadata": {
        "id": "1ginttgkiqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "uewm7emxvde",
      "source": [
        "## Hybrid Evaluation: Automated Metrics + Human Review\n",
        "\n",
        "After training, evaluate your model using a hybrid approach:\n",
        "- **Automated metrics** (BLEU, exact match) for questions where you have reference solutions\n",
        "- **Quality checks** for all generated solutions\n",
        "- **Human review** for questions without reference solutions\n",
        "\n",
        "This section will:\n",
        "1. Add your reference solutions to the test data\n",
        "2. Run evaluation (automated + quality checks)\n",
        "3. Generate results in JSON and CSV formats for review"
      ],
      "metadata": {
        "id": "uewm7emxvde"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}